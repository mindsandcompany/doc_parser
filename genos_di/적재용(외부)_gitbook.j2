# ì§€ëŠ¥í˜• ë¬¸ì„œ ì „ì²˜ë¦¬ê¸° - ì ì¬ìš©(ì™¸ë¶€)

ì™¸ë¶€ PDF ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì ì¬í•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬ê¸°ì…ë‹ˆë‹¤. OCR ìë™ íŒë‹¨ ê¸°ëŠ¥ê³¼ PaddleOCRì„ í†µí•´ ìŠ¤ìº” ë¬¸ì„œì™€ ë””ì§€í„¸ ë¬¸ì„œë¥¼ ëª¨ë‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.

## ğŸ”§ ê³µí†µ ì»´í¬ë„ŒíŠ¸

### GenOSVectorMeta
ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸ì…ë‹ˆë‹¤.

```python
class GenOSVectorMeta(BaseModel):
    class Config:
        extra = 'allow'
    
    text: str = None           # ì²­í¬ í…ìŠ¤íŠ¸
    n_char: int = None         # ë¬¸ì ìˆ˜
    n_word: int = None         # ë‹¨ì–´ ìˆ˜
    n_line: int = None         # ì¤„ ìˆ˜
    i_page: int = None         # í˜ì´ì§€ ë²ˆí˜¸
    i_chunk_on_page: int = None    # í˜ì´ì§€ ë‚´ ì²­í¬ ì¸ë±ìŠ¤
    n_chunk_of_page: int = None    # í˜ì´ì§€ ë‚´ ì´ ì²­í¬ ìˆ˜
    i_chunk_on_doc: int = None     # ë¬¸ì„œ ë‚´ ì²­í¬ ì¸ë±ìŠ¤
    n_chunk_of_doc: int = None     # ë¬¸ì„œ ë‚´ ì´ ì²­í¬ ìˆ˜
    n_page: int = None         # ì´ í˜ì´ì§€ ìˆ˜
    reg_date: str = None       # ë“±ë¡ì¼ì‹œ (ISO 8601)
    chunk_bboxes: str = None   # JSON ë¬¸ìì—´ - ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´
    media_files: str = None    # JSON ë¬¸ìì—´ - ë¯¸ë””ì–´ íŒŒì¼ ì •ë³´
    created_date: int = None   # ì‘ì„±ì¼ (YYYYMMDD í˜•ì‹) â˜…BOK íŠ¹í™”
    authors: str = None        # JSON ë¬¸ìì—´ - ì‘ì„±ì ë¦¬ìŠ¤íŠ¸
    title: str = None          # ë¬¸ì„œ ì œëª©
```

### GenOSVectorMetaBuilder
ë¹Œë” íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

```python
class GenOSVectorMetaBuilder:
    def set_text(self, text: str) -> "GenOSVectorMetaBuilder":
        """í…ìŠ¤íŠ¸ì™€ ê´€ë ¨ í†µê³„ ì„¤ì •"""
        self.text = text
        self.n_char = len(text)
        self.n_word = len(text.split())
        self.n_line = len(text.splitlines())
        return self
    
    def set_chunk_bboxes(self, doc_items: list, document: DoclingDocument):
        """ì²­í¬ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ë¥¼ ìƒëŒ€ ì¢Œí‘œë¡œ ì €ì¥"""
        chunk_bboxes = []
        for item in doc_items:
            for prov in item.prov:
                bbox_data = {
                    'l': bbox.l / size.width,   # ì™¼ìª½ (0-1)
                    't': bbox.t / size.height,  # ìƒë‹¨ (0-1)
                    'r': bbox.r / size.width,   # ì˜¤ë¥¸ìª½ (0-1)
                    'b': bbox.b / size.height,  # í•˜ë‹¨ (0-1)
                    'coord_origin': bbox.coord_origin.value
                }
                chunk_bboxes.append({
                    'page': prov.page_no,
                    'bbox': bbox_data,
                    'type': item.label,
                    'ref': item.self_ref
                })
        self.chunk_bboxes = json.dumps(chunk_bboxes)
        return self
```

### DocumentProcessor
ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
class DocumentProcessor:
    def __init__(self):
        # ê¸°ë³¸ PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        pipe_line_options = PdfPipelineOptions()
        pipe_line_options.generate_page_images = True
        pipe_line_options.generate_picture_images = True
        pipe_line_options.do_ocr = False  # ê¸°ë³¸ì ìœ¼ë¡œ OCR ë¹„í™œì„±í™”
        pipe_line_options.do_table_structure = True
        pipe_line_options.table_structure_options.mode = TableFormerMode.ACCURATE
        
        # Primary ì»¨ë²„í„° (DoclingParseV4)
        self.converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipe_line_options,
                    backend=DoclingParseV4DocumentBackend
                )
            }
        )
        
        # Fallback ì»¨ë²„í„° (PyPdfium)
        self.second_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipe_line_options,
                    backend=PyPdfiumDocumentBackend
                )
            }
        )
        
        # OCR ì „ìš© ì»¨ë²„í„° (PaddleOCR)
        ocr_options = PaddleOcrOptions(
            force_full_page_ocr=True,
            lang=['korean'],
            text_score=0.3  # í…ìŠ¤íŠ¸ ì‹ ë¢°ë„ ì„ê³„ê°’
        )
        self.ocr_pipe_line_options = pipe_line_options.model_copy(deep=True)
        self.ocr_pipe_line_options.do_ocr = True
        self.ocr_pipe_line_options.ocr_options = ocr_options
        
        self.ocr_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=self.ocr_pipe_line_options,
                    backend=DoclingParseV4DocumentBackend
                )
            }
        )
```

### HierarchicalChunker & HybridChunker
ë¬¸ì„œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í† í° ì œí•œì„ ê³ ë ¤í•œ ì²­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
class HybridChunker(BaseChunker):
    tokenizer: str = "sentence-transformers/all-MiniLM-L6-v2"
    max_tokens: int = 1000  # ì ì¬ìš©(ì™¸ë¶€)ëŠ” 1000 í† í° ì‚¬ìš©
    merge_peers: bool = True
```

## ğŸ“‚ ì „ì²˜ë¦¬ íë¦„

### 1. ë¬¸ì„œ ë¡œë“œ ë° OCR ìë™ íŒë‹¨
```python
def load_documents_with_docling(self, file_path: str, **kwargs) -> DoclingDocument:
    try:
        # 1ì°¨: DoclingParseV4ë¡œ ì‹œë„
        conv_result = self.converter.convert(file_path, raises_on_error=True)
    except Exception:
        # 2ì°¨: PyPdfiumìœ¼ë¡œ í´ë°±
        conv_result = self.second_converter.convert(file_path, raises_on_error=True)
    
    document = conv_result.document
    
    # OCR í•„ìš”ì„± ìë™ ì²´í¬
    if not check_document(document, self.enrichment_options):
        # í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ OCR ìˆ˜í–‰
        document = self.ocr_converter.convert(file_path, raises_on_error=True).document
    
    return document
```

### 2. ë¬¸ì„œ Metadata Enrichment
```python
def enrichment(self, document: DoclingDocument, **kwargs) -> DoclingDocument:
    # LLMì„ í†µí•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ
    # extract_metadata=Trueë¡œ ì‘ì„±ì¼(created_date) ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œ
    enrichment_options = DataEnrichmentOptions(
        do_toc_enrichment=False,         # ëª©ì°¨ ìƒì„± í™œì„±í™”
        extract_metadata=True,           # â˜…ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í™œì„±í™” (ì‘ì„±ì¼ ì¶”ì¶œ)
        metadata_api_provider="custom",  # ë©”íƒ€ë°ì´í„° API í”„ë¡œë°”ì´ë”
        metadata_api_base_url="http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions",
        metadata_api_key="9e32423947fd4a5da07a28962fe88487",
        metadata_model="/model/snapshots/9eb2daaa8597bf192a8b0e73f848f3a102794df5",
        toc_api_provider="custom",
        toc_api_base_url="http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions",
        toc_api_key="9e32423947fd4a5da07a28962fe88487",
        toc_model="/model/snapshots/9eb2daaa8597bf192a8b0e73f848f3a102794df5",
        toc_temperature=0.0,             # ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼
        toc_top_p=0,
        toc_seed=33,                     # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        toc_max_tokens=1000
    )
    
    # enrich_documentëŠ” ë¬¸ì„œì—ì„œ ì‘ì„±ì¼, ì‘ì„±ì ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ LLMìœ¼ë¡œ ì¶”ì¶œ
    # ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°ëŠ” document.key_value_itemsì— ì €ì¥ë¨
    document = enrich_document(document, enrichment_options)
    return document
```

### 3. ë¬¸ì„œ ì²­í‚¹
```python
def split_documents(self, documents: DoclingDocument, **kwargs) -> List[DocChunk]:
    chunker = HybridChunker(
        max_tokens=1000,
        merge_peers=True
    )
    chunks = list(chunker.chunk(dl_doc=documents, **kwargs))
    
    # í˜ì´ì§€ë³„ ì²­í¬ ìˆ˜ ê³„ì‚°
    for chunk in chunks:
        self.page_chunk_counts[chunk.meta.doc_items[0].prov[0].page_no] += 1
    
    return chunks
```

### 4. ë²¡í„° ë©”íƒ€ë°ì´í„° ìƒì„±
```python
async def compose_vectors(self, document: DoclingDocument, chunks: List[DocChunk], 
                         file_path: str, request: Request, **kwargs) -> list[dict]:
    # Enrichmentì—ì„œ ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°(ì‘ì„±ì¼)ë¥¼ ê°€ì ¸ì˜´ â˜…BOK íŠ¹í™”
    # document.key_value_itemsì— LLMì´ ì¶”ì¶œí•œ ì‘ì„±ì¼ ì •ë³´ê°€ ì €ì¥ë¨
    created_date = 0
    if (document.key_value_items and len(document.key_value_items) > 0 and
        hasattr(document.key_value_items[0], 'graph')):
        # metadata enrichmentë¡œ ì¶”ì¶œëœ ì‘ì„±ì¼ í…ìŠ¤íŠ¸ â˜…BOK íŠ¹í™”
        date_text = document.key_value_items[0].graph.cells[1].text
        created_date = self.parse_created_date(date_text)
    
    # ì‘ì„±ì ì •ë³´ íŒŒì‹±
    authors = ""
    if "authors" in kwargs:
        authors = json.dumps(self.parse_authors(kwargs["authors"]))
    
    # ì œëª© ì¶”ì¶œ
    title = ""
    for item, _ in document.iterate_items():
        if hasattr(item, 'label') and item.label == DocItemLabel.TITLE:
            title = item.text.strip() if item.text else ""
            break
    
    # ê¸€ë¡œë²Œ ë©”íƒ€ë°ì´í„°
    global_metadata = dict(
        n_chunk_of_doc=len(chunks),
        n_page=document.num_pages(),
        reg_date=datetime.now().isoformat(timespec='seconds') + 'Z',
        created_date=created_date,  # metadata enrichmentë¡œ ì¶”ì¶œëœ ì‘ì„±ì¼ â˜…BOK íŠ¹í™”
        authors=authors,
        title=title
    )
    
    # ê° ì²­í¬ë³„ ë²¡í„° ìƒì„±
    vectors = []
    for chunk_idx, chunk in enumerate(chunks):
        vector = (GenOSVectorMetaBuilder()
                  .set_text(content)
                  .set_page_info(chunk_page, chunk_index_on_page, self.page_chunk_counts[chunk_page])
                  .set_chunk_index(chunk_idx)
                  .set_global_metadata(**global_metadata)
                  .set_chunk_bboxes(chunk.meta.doc_items, document)
                  .set_media_files(chunk.meta.doc_items)
                  ).build()
        vectors.append(vector)
    
    return vectors
```

### 5. ì‘ì„±ì¼ íŒŒì‹± (BOK íŠ¹í™”)
```python
def parse_created_date(self, date_text: str) -> Optional[int]:
    """ì‘ì„±ì¼ì„ YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ë¡œ ë³€í™˜ â˜…BOK íŠ¹í™”"""
    
    if not date_text or date_text == "None":
        return 0
    
    # YYYY-MM-DD í˜•ì‹
    match_full = re.match(r'^(\d{4})-(\d{1,2})-(\d{1,2})$', date_text)
    if match_full:
        year, month, day = match_full.groups()
        return int(f"{year}{month.zfill(2)}{day.zfill(2)}")
    
    # YYYY-MM í˜•ì‹ (ì¼ìëŠ” 01ë¡œ ì„¤ì •)
    match_month = re.match(r'^(\d{4})-(\d{1,2})$', date_text)
    if match_month:
        year, month = match_month.groups()
        return int(f"{year}{month.zfill(2)}01")
    
    # YYYY í˜•ì‹ (ì›”ì¼ì€ 0101ë¡œ ì„¤ì •)
    match_year = re.match(r'^(\d{4})$', date_text)
    if match_year:
        year = match_year.group(1)
        return int(f"{year}0101")
    
    return 0
```

### 6. ì‘ì„±ì ì •ë³´ íŒŒì‹±
```python
def parse_authors(self, authors_data) -> list[str]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì‘ì„±ì ì •ë³´ë¥¼ í†µì¼ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    
    if isinstance(authors_data, list):
        names = []
        for author in authors_data:
            if isinstance(author, dict):
                # "ì´ë¦„" ë˜ëŠ” "name" í‚¤ ì°¾ê¸°
                if "ì´ë¦„" in author:
                    names.append(author["ì´ë¦„"].strip())
                elif "name" in author:
                    names.append(author["name"].strip())
            elif isinstance(author, str):
                names.append(author.strip())
        return list(set(names))  # ì¤‘ë³µ ì œê±°
    
    elif isinstance(authors_data, str):
        # êµ¬ë¶„ì: , ; / \n Â· â€¢
        separators = [',', ';', '/', '\n', 'Â·', 'â€¢']
        for sep in separators:
            if sep in authors_data:
                names = [name.strip() for name in authors_data.split(sep)]
                return list(set(names))
        return [authors_data.strip()] if authors_data.strip() else []
    
    return []
```

## âœ¨ ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§• í¬ì¸íŠ¸

### OCR ì˜µì…˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•
```python
# ë‹¤êµ­ì–´ OCR ì„¤ì •
ocr_options = PaddleOcrOptions(
    force_full_page_ocr=True,
    lang=['korean', 'en'],     # í•œêµ­ì–´ì™€ ì˜ì–´
    text_score=0.5              # ë” ì—„ê²©í•œ ì‹ ë¢°ë„
)

# ì¼ë³¸ì–´ ë¬¸ì„œ ì„¤ì •
ocr_options = PaddleOcrOptions(
    force_full_page_ocr=True,
    lang=['japan'],
    text_score=0.3
)
```

### OCR ê°•ì œ ì‹¤í–‰
```python
# check_document ê±´ë„ˆë›°ê³  OCR ê°•ì œ ì‹¤í–‰
document = processor.ocr_converter.convert(file_path, raises_on_error=True).document
```

## âœ… ìœ ì§€ë³´ìˆ˜ íŒ

### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
1. **OCR ê²°ê³¼ ë¶€ì •í™•**: text_score ê°’ ì¡°ì •
2. **ì²˜ë¦¬ ì‹œê°„ ê³¼ë‹¤**: GPU ì‚¬ìš© í™•ì¸, ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ ê°ì†Œ

### ì£¼ìš” ì°¨ì´ì  (ê¸°ë³¸ ì ì¬ìš©(ì™¸ë¶€) ëŒ€ë¹„)
- **OCR ìë™ íŒë‹¨**: check_document í•¨ìˆ˜ë¡œ OCR í•„ìš”ì„± ìë™ ê°ì§€
- **PaddleOCR ì‚¬ìš©**: Tesseract ëŒ€ì‹  PaddleOCR ì‚¬ìš© (í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜)

