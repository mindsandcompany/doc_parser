# ì§€ëŠ¥í˜• ë¬¸ì„œ ì „ì²˜ë¦¬ê¸° - ì ì¬ìš©(ë‚´ë¶€)

BOK(í•œêµ­ì€í–‰) JSON í˜•ì‹ì˜ ë‚´ë¶€ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì ì¬í•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬ê¸°ì…ë‹ˆë‹¤. ì¡°ì§ ë‚´ë¶€ ë©”íƒ€ë°ì´í„°(íŒ€, ë¶€ì„œ)ë¥¼ ì¶”ì¶œí•˜ê³  ë¬¸ì„œ êµ¬ì¡°ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.

## ğŸ”§ ê³µí†µ ì»´í¬ë„ŒíŠ¸

### GenOSVectorMeta
ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸ì…ë‹ˆë‹¤.

```python
class GenOSVectorMeta(BaseModel):
    class Config:
        extra = 'allow'
    
    text: str = None           # ì²­í¬ í…ìŠ¤íŠ¸
    n_char: int = None         # ë¬¸ì ìˆ˜
    n_word: int = None         # ë‹¨ì–´ ìˆ˜
    n_line: int = None         # ì¤„ ìˆ˜
    i_page: int = None         # í˜ì´ì§€ ë²ˆí˜¸
    i_chunk_on_page: int = None    # í˜ì´ì§€ ë‚´ ì²­í¬ ì¸ë±ìŠ¤
    n_chunk_of_page: int = None    # í˜ì´ì§€ ë‚´ ì´ ì²­í¬ ìˆ˜
    i_chunk_on_doc: int = None     # ë¬¸ì„œ ë‚´ ì²­í¬ ì¸ë±ìŠ¤
    n_chunk_of_doc: int = None     # ë¬¸ì„œ ë‚´ ì´ ì²­í¬ ìˆ˜
    n_page: int = None         # ì´ í˜ì´ì§€ ìˆ˜
    reg_date: str = None       # ë“±ë¡ì¼ì‹œ (ISO 8601)
    chunk_bboxes: str = None   # JSON ë¬¸ìì—´ - ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´
    media_files: str = None    # JSON ë¬¸ìì—´ - ë¯¸ë””ì–´ íŒŒì¼ ì •ë³´
    created_date: int = None   # ì‘ì„±ì¼ (YYYYMMDD í˜•ì‹) â˜…BOK ì „ìš©
    authors_team: str = None   # JSON ë¬¸ìì—´ - íŒ€ ë¦¬ìŠ¤íŠ¸ â˜…BOK ì „ìš©
    authors_department: str = None  # JSON ë¬¸ìì—´ - ë¶€ì„œ ë¦¬ìŠ¤íŠ¸ â˜…BOK ì „ìš©
    title: str = None          # ë¬¸ì„œ ì œëª©
```

### GenOSVectorMetaBuilder
ë¹Œë” íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

```python
class GenOSVectorMetaBuilder:
    def set_text(self, text: str) -> "GenOSVectorMetaBuilder":
        """í…ìŠ¤íŠ¸ì™€ ê´€ë ¨ í†µê³„ ì„¤ì •"""
        self.text = text
        self.n_char = len(text)
        self.n_word = len(text.split())
        self.n_line = len(text.splitlines())
        return self
    
    def set_global_metadata(self, **global_metadata) -> "GenOSVectorMetaBuilder":
        """ê¸€ë¡œë²Œ ë©”íƒ€ë°ì´í„° ë³‘í•© (íŒ€, ë¶€ì„œ ì •ë³´ í¬í•¨)"""
        for key, value in global_metadata.items():
            if hasattr(self, key):
                setattr(self, key, value)
        return self
    
    def set_chunk_bboxes(self, doc_items: list, document: DoclingDocument):
        """ì²­í¬ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ë¥¼ ìƒëŒ€ ì¢Œí‘œë¡œ ì €ì¥"""
        chunk_bboxes = []
        for item in doc_items:
            for prov in item.prov:
                bbox_data = {
                    'l': bbox.l / size.width,
                    't': bbox.t / size.height,
                    'r': bbox.r / size.width,
                    'b': bbox.b / size.height,
                    'coord_origin': bbox.coord_origin.value
                }
                chunk_bboxes.append({
                    'page': prov.page_no,
                    'bbox': bbox_data,
                    'type': item.label,
                    'ref': item.self_ref
                })
        self.chunk_bboxes = json.dumps(chunk_bboxes)
        return self
```

### DocumentProcessor
BOK JSON ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
class DocumentProcessor:
    def __init__(self):
        self.page_chunk_counts = defaultdict(int)
        self.simple_pipeline_options = PipelineOptions()
        self.simple_pipeline_options.save_images = False  # ê¸°ë³¸ê°’
        
        # BOK JSON í˜•ì‹ ì „ìš© ì»¨ë²„í„°
        self.converter = DocumentConverter(
            format_options={
                InputFormat.JSON_DOCLING: BOKJsonFormatOption(
                    pipeline_options=self.simple_pipeline_options,
                )
            }
        )
    
    def _create_converters(self):
        """save_images ì˜µì…˜ ë³€ê²½ ì‹œ ì»¨ë²„í„° ì¬ìƒì„±"""
        self.converter = DocumentConverter(
            format_options={
                InputFormat.JSON_DOCLING: BOKJsonFormatOption(
                    pipeline_options=self.simple_pipeline_options,
                )
            }
        )
```

### HierarchicalChunker & HybridChunker
ë¬¸ì„œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í† í° ì œí•œì„ ê³ ë ¤í•œ ì²­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
class HierarchicalChunker(BaseChunker):
    """ë¬¸ì„œ êµ¬ì¡°ì™€ í—¤ë” ê³„ì¸µì„ ìœ ì§€í•˜ë©´ì„œ ì•„ì´í…œì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
    merge_list_items: bool = True
    
    def chunk(self, dl_doc: DLDocument, **kwargs) -> Iterator[BaseChunk]:
        # í—¤ë” ë ˆë²¨ ê´€ë¦¬
        current_heading_by_level: dict[LevelNumber, str] = {}
        
        # ì„¹ì…˜ í—¤ë” ì²˜ë¦¬
        if item.label == DocItemLabel.TITLE:
            header_level = 0
        elif item.label == DocItemLabel.SECTION_HEADER:
            header_level = 1
        else:
            header_level = item.level
        
        # í•˜ìœ„ ë ˆë²¨ í—¤ë” ìë™ ì œê±°
        keys_to_del = [k for k in current_heading_by_level if k > header_level]
        for k in keys_to_del:
            current_heading_by_level.pop(k, None)

class HybridChunker(BaseChunker):
    tokenizer: str = "sentence-transformers/all-MiniLM-L6-v2"
    max_tokens: int = 1000  # BOK ë¬¸ì„œë„ 1000 í† í° ì‚¬ìš©
    merge_peers: bool = True
```

## ğŸ“‚ ì „ì²˜ë¦¬ íë¦„

### 1. BOK JSON ë¬¸ì„œ ë¡œë“œ
```python
def load_documents_with_docling(self, file_path: str, **kwargs: dict) -> DoclingDocument:
    save_images = kwargs.get('save_images', False)
    
    # save_images ì˜µì…˜ì´ ë³€ê²½ë˜ë©´ ì»¨ë²„í„° ì¬ìƒì„±
    if self.simple_pipeline_options.save_images != save_images:
        self.simple_pipeline_options.save_images = save_images
        self._create_converters()
    
    # BOK JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    conv_result: ConversionResult = self.converter.convert(file_path, raises_on_error=True)
    return conv_result.document
```

### 2. ë¬¸ì„œ Metadata Enrichment
```python
def enrichment(self, document: DoclingDocument, **kwargs: dict) -> DoclingDocument:
    # LLMì„ í†µí•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ
    # extract_metadata=Trueë¡œ ì‘ì„±ì¼(created_date) ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œ
    enrichment_options = DataEnrichmentOptions(
        do_toc_enrichment=False,         # ëª©ì°¨ ìƒì„± í™œì„±í™”
        extract_metadata=True,           # â˜…ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í™œì„±í™” (ì‘ì„±ì¼ ì¶”ì¶œ)
        metadata_api_provider="custom",  # ë©”íƒ€ë°ì´í„° API í”„ë¡œë°”ì´ë”
        metadata_api_base_url="http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions",
        metadata_api_key="9e32423947fd4a5da07a28962fe88487",
        metadata_model="/model/snapshots/9eb2daaa8597bf192a8b0e73f848f3a102794df5",
        toc_api_provider="custom",
        toc_api_base_url="http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions",
        toc_api_key="9e32423947fd4a5da07a28962fe88487",
        toc_model="/model/snapshots/9eb2daaa8597bf192a8b0e73f848f3a102794df5",
        toc_temperature=0.0,             # ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼
        toc_top_p=0,
        toc_seed=33,                     # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        toc_max_tokens=1000
    )
    
    # enrich_documentëŠ” ë¬¸ì„œì—ì„œ ì‘ì„±ì¼, ì‘ì„±ì ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ LLMìœ¼ë¡œ ì¶”ì¶œ
    # ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°ëŠ” document.key_value_itemsì— ì €ì¥ë¨
    document = enrich_document(document, enrichment_options)
    return document
```

### 3. ë¬¸ì„œ ì²­í‚¹
```python
def split_documents(self, documents: DoclingDocument, **kwargs: dict) -> List[DocChunk]:
    chunker = HybridChunker(
        max_tokens=1000,
        merge_peers=True
    )
    chunks = list(chunker.chunk(dl_doc=documents, **kwargs))
    
    # í˜ì´ì§€ë³„ ì²­í¬ ìˆ˜ ê³„ì‚°
    for chunk in chunks:
        self.page_chunk_counts[chunk.meta.doc_items[0].prov[0].page_no] += 1
    
    return chunks
```

### 4. ë²¡í„° ë©”íƒ€ë°ì´í„° ìƒì„± (BOK íŠ¹í™”)
```python
async def compose_vectors(self, document: DoclingDocument, chunks: List[DocChunk], 
                         file_path: str, request: Request, **kwargs: dict) -> list[dict]:
    # Enrichmentì—ì„œ ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°(ì‘ì„±ì¼)ë¥¼ ê°€ì ¸ì˜´
    # document.key_value_itemsì— LLMì´ ì¶”ì¶œí•œ ì‘ì„±ì¼ ì •ë³´ê°€ ì €ì¥ë¨
    created_date = 0
    if (document.key_value_items and len(document.key_value_items) > 0 and
        hasattr(document.key_value_items[0], 'graph')):
        # metadata enrichmentë¡œ ì¶”ì¶œëœ ì‘ì„±ì¼ í…ìŠ¤íŠ¸
        date_text = document.key_value_items[0].graph.cells[1].text
        created_date = self.parse_created_date(date_text)
    
    # íŒ€/ë¶€ì„œ ì •ë³´ ì¶”ì¶œ â˜…BOK ì „ìš©
    authors_team = ""
    authors_department = ""
    if "authors_team" in kwargs:
        authors_team = json.dumps(kwargs["authors_team"])
    if "authors_department" in kwargs:
        authors_department = json.dumps(kwargs["authors_department"])
    
    # ì œëª© ì¶”ì¶œ
    title = ""
    for item, _ in document.iterate_items():
        if hasattr(item, 'label') and item.label == DocItemLabel.TITLE:
            title = item.text.strip() if item.text else ""
            break
    
    # ê¸€ë¡œë²Œ ë©”íƒ€ë°ì´í„°
    global_metadata = dict(
        n_chunk_of_doc=len(chunks),
        n_page=document.num_pages(),
        reg_date=datetime.now().isoformat(timespec='seconds') + 'Z',
        created_date=created_date,          # â˜…BOK ì „ìš©
        authors_team=authors_team,          # â˜…BOK ì „ìš©
        authors_department=authors_department,  # â˜…BOK ì „ìš©
        title=title
    )
    
    # ê° ì²­í¬ë³„ ë²¡í„° ìƒì„±
    vectors = []
    for chunk_idx, chunk in enumerate(chunks):
        vector = (GenOSVectorMetaBuilder()
                  .set_text(content)
                  .set_page_info(chunk_page, chunk_index_on_page, self.page_chunk_counts[chunk_page])
                  .set_chunk_index(chunk_idx)
                  .set_global_metadata(**global_metadata)  # íŒ€/ë¶€ì„œ ì •ë³´ í¬í•¨
                  .set_chunk_bboxes(chunk.meta.doc_items, document)
                  .set_media_files(chunk.meta.doc_items)
                  ).build()
        vectors.append(vector)
    
    # ë¯¸ë””ì–´ íŒŒì¼ ë¹„ë™ê¸° ì—…ë¡œë“œ
    await asyncio.gather(*upload_tasks)
    
    return vectors
```

### 5. ì‘ì„±ì¼ íŒŒì‹± (BOK ì „ìš©)
```python
def parse_created_date(self, date_text: str) -> Optional[int]:
    """ì‘ì„±ì¼ì„ YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ë¡œ ë³€í™˜"""
    
    if not date_text or date_text == "None":
        return 0
    
    # YYYY-MM-DD í˜•ì‹
    match_full = re.match(r'^(\d{4})-(\d{1,2})-(\d{1,2})$', date_text)
    if match_full:
        year, month, day = match_full.groups()
        return int(f"{year}{month.zfill(2)}{day.zfill(2)}")
    
    # YYYY-MM í˜•ì‹ (ì¼ìëŠ” 01ë¡œ ì„¤ì •)
    match_month = re.match(r'^(\d{4})-(\d{1,2})$', date_text)
    if match_month:
        year, month = match_month.groups()
        return int(f"{year}{month.zfill(2)}01")
    
    # YYYY í˜•ì‹ (ì›”ì¼ì€ 0101ë¡œ ì„¤ì •)
    match_year = re.match(r'^(\d{4})$', date_text)
    if match_year:
        year = match_year.group(1)
        return int(f"{year}0101")
    
    return 0
```

### 6. kwargsë¥¼ ì„ì‹œ JSON íŒŒì¼ë¡œ ì²˜ë¦¬ (BOK íŠ¹í™”)
```python
async def __call__(self, request: Request, file_path: str, **kwargs: dict):
    # kwargsë¥¼ ì„ì‹œ JSON íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as temp_file:
        json.dump(kwargs, temp_file, ensure_ascii=False, indent=2)
        temp_file_path = temp_file.name
    
    try:
        # BOK JSON í˜•ì‹ìœ¼ë¡œ ë¡œë“œ
        document = self.load_documents(temp_file_path, **kwargs)
        
        # ì´ë¯¸ì§€ ì°¸ì¡° ì„¤ì •
        document = document._with_pictures_refs(image_dir=artifacts_dir, reference_path=reference_path)
        
        # Enrichment
        document = self.enrichment(document, **kwargs)
        
        # ì²­í‚¹ ë° ë²¡í„° ìƒì„±
        chunks = self.split_documents(document, **kwargs)
        vectors = await self.compose_vectors(document, chunks, file_path, request, **kwargs)
        
        return vectors
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
```

## âœ¨ ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§• í¬ì¸íŠ¸

### íŒ€/ë¶€ì„œ ì •ë³´ ì¶”ê°€
```python
# íŒ€ê³¼ ë¶€ì„œ ì •ë³´ë¥¼ kwargsë¡œ ì „ë‹¬
vectors = await processor(
    request=request,
    file_path="report.json",
    authors_team=["ê²½ì œë¶„ì„íŒ€", "ê¸ˆìœµì‹œì¥íŒ€"],
    authors_department=["ì—°êµ¬ë¶€", "ì¡°ì‚¬ë¶€"]
)
```

## âœ… ìœ ì§€ë³´ìˆ˜ íŒ

### íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
1. **ì²­í¬ ìƒì„± ì‹¤íŒ¨**: BOK JSON í˜•ì‹ ê²€ì¦
2. **ë©”íƒ€ë°ì´í„° ëˆ„ë½**: key_value_items êµ¬ì¡° í™•ì¸
3. **ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜**: ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›

### ì£¼ìš” ì°¨ì´ì  (ì ì¬ìš©(ì™¸ë¶€) ëŒ€ë¹„)
- **BOK JSON ì „ìš©**: PDFê°€ ì•„ë‹Œ JSON_DOCLING í˜•ì‹ ì‚¬ìš©
- **kwargs ì²˜ë¦¬**: ì„ì‹œ JSON íŒŒì¼ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬

