---
description: >-
  Genos ëŠ” í¬ê²Œ ì ì¬ìš©(ë‚´ë¶€), ì ì¬ìš©(ì™¸ë¶€), ì ì¬ìš©(ê·œì •), ì²¨ë¶€ìš© 4ê°€ì§€ ìœ í˜•ì˜ ì „ì²˜ë¦¬ê¸° (document parser)ë¥¼
  ì§€ì›í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì ì¬ìš©(ê·œì •) Intelligent Doc Parser - ì˜ë¯¸ê¸°ë°˜ ì²­í‚¹ ì „ì²˜ë¦¬ê¸°ì˜ ì½”ë“œ ì›í˜•ì— ëŒ€í•´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.
icon: forward
---

# ì ì¬ìš©(ê·œì •) ì§€ëŠ¥í˜• ë¬¸ì„œ ì „ì²˜ë¦¬ê¸°

<figure><img src="../../../../.gitbook/assets/preprocess_code.png" alt=""><figcaption><p>ì „ì²˜ë¦¬ê¸° ìƒì„¸ì—ì„œ ì•„ë˜ ì½”ë“œë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></figcaption></figure>

***

ì—¬ê¸°ì„œëŠ” Genos ì ì¬ìš©(ê·œì •) ì§€ëŠ¥í˜• ë¬¸ì„œ íŒŒì„œì˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë‚´ ì£¼ìš” êµ¬ì„± ìš”ì†Œì— ëŒ€í•œ ì½”ë“œ ì¤‘ì‹¬ì˜ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. ì½”ë“œ ì¡°ê°ê³¼ í•¨ê»˜ ê° ë¶€ë¶„ì˜ ê¸°ëŠ¥ì„ ì´í•´í•¨ìœ¼ë¡œì¨, íŠ¹ì • ìš”êµ¬ ì‚¬í•­ ë° ë¬¸ì„œ ìœ í˜•ì— ë§ê²Œ ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”§ ê³µí†µ êµ¬ì„±ìš”ì†Œ

#### `HierarchicalChunker` ë° `HybridChunker`

`HierarchicalChunker`ëŠ” ë¬¸ì„œë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” ì—­í• ì„ í•˜ë©°, `HybridChunker`ëŠ” í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì„¹ì…˜ë³„ ì²­í¬ë¥¼ ë¶„í• í•˜ê³  ë³‘í•©í•˜ëŠ” ê³ ê¸‰ ì²­ì»¤ì…ë‹ˆë‹¤.

```python
class HierarchicalChunker(BaseChunker):
    merge_list_items: bool = True

    def chunk(self, dl_doc: DLDocument, **kwargs: Any) -> Iterator[BaseChunk]:
        # ëª¨ë“  ì•„ì´í…œê³¼ í—¤ë” ì •ë³´ ìˆ˜ì§‘
        all_items = []
        all_header_info = []  # ê° ì•„ì´í…œì˜ í—¤ë” ì •ë³´
        current_heading_by_level: dict[LevelNumber, str] = {}
        all_header_short_info = []  # ê° ì•„ì´í…œì˜ ì§§ì€ í—¤ë” ì •ë³´
        current_heading_short_by_level: dict[LevelNumber, str] = {}
        list_items: list[TextItem] = []

        # ëª¨ë“  ì•„ì´í…œ ìˆœíšŒí•˜ë©° í—¤ë” ì •ë³´ ì¶”ì 
        for item, level in dl_doc.iterate_items():
            # ì„¹ì…˜ í—¤ë” ì²˜ë¦¬
            if isinstance(item, SectionHeaderItem) or (
                isinstance(item, TextItem) and
                item.label in [DocItemLabel.SECTION_HEADER, DocItemLabel.TITLE]
            ):
                header_level = (
                    item.level if isinstance(item, SectionHeaderItem)
                    else (0 if item.label == DocItemLabel.TITLE else 1)
                )
                current_heading_by_level[header_level] = item.text
                current_heading_short_by_level[header_level] = item.orig  # ì²« ë‹¨ì–´ë¡œ ì§§ì€ í—¤ë” ì •ë³´ ì„¤ì •

                # ... í—¤ë” ì²˜ë¦¬ ë¡œì§

        # ëª¨ë“  ì•„ì´í…œì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë°˜í™˜ (HybridChunkerì—ì„œ ë¶„í• )
        # headingsëŠ” Noneìœ¼ë¡œ ì„¤ì •í•˜ê³ , í—¤ë” ì •ë³´ëŠ” ë³„ë„ë¡œ ê´€ë¦¬
        chunk = DocChunk(
            text="",  # í…ìŠ¤íŠ¸ëŠ” HybridChunkerì—ì„œ ìƒì„±
            meta=DocMeta(
                doc_items=all_items,
                headings=None,  # DocMetaì˜ ì›ë˜ í˜•ì‹ ìœ ì§€
                captions=None,
                origin=dl_doc.origin,
            ),
        )

        # ì²­í¬ì— ë‘ ê°€ì§€ í—¤ë” ì •ë³´ ëª¨ë‘ ì €ì¥
        chunk._header_info_list = all_header_info
        chunk._header_short_info_list = all_header_short_info  # ì§§ì€ í—¤ë” ì •ë³´ë„ ì €ì¥
        yield chunk

class HybridChunker(BaseChunker):

    def chunk(self, dl_doc: DoclingDocument, **kwargs: Any) -> Iterator[BaseChunk]:
        """ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ì—¬ ë°˜í™˜

        Args:
            dl_doc: ì²­í‚¹í•  ë¬¸ì„œ

        Yields:
            í† í° ì œí•œì— ë§ê²Œ ë¶„í• ëœ ì²­í¬ë“¤
        """
        doc_chunks = list(self._inner_chunker.chunk(dl_doc=dl_doc, **kwargs))

        if not doc_chunks:
            return iter([])

        doc_chunk = doc_chunks[0]  # HierarchicalChunkerëŠ” í•˜ë‚˜ì˜ ì²­í¬ë§Œ ë°˜í™˜

        final_chunks = self._split_document_by_tokens(doc_chunk, dl_doc)

        return iter(final_chunks)
```

#### `GenOSVectorMetaBuilder` ë° `GenOSVectorMeta`

`GenOSVectorMetaBuilder`ëŠ” ê° ì²­í¬ì— ëŒ€í•œ ìƒì„¸ ë©”íƒ€ë°ì´í„° ê°ì²´ì¸ `GenOSVectorMeta`ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

**`GenOSVectorMeta` (Pydantic ëª¨ë¸)**

ë¨¼ì €, ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ë  ë©”íƒ€ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸ì…ë‹ˆë‹¤.

Python

```python
class GenOSVectorMeta(BaseModel):
    class Config:
        extra = 'allow' # Pydantic v2ì—ì„œëŠ” extra='allow' ëŒ€ì‹  model_config ì‚¬ìš© ê°€ëŠ¥

    text: str = None
    n_char: int = None
    n_word: int = None
    n_line: int = None
    e_page: int = None
    i_page: int = None
    i_chunk_on_page: int = None
    n_chunk_of_page: int = None
    i_chunk_on_doc: int = None
    n_chunk_of_doc: int = None
    n_page: int = None
    reg_date: str = None
    chunk_bboxes: str = None
    media_files: str = None
    title: str = None
    created_date: int = None
```

**ì„¤ëª…:**

* `BaseModel`ì„ ìƒì†ë°›ì•„ Pydantic ëª¨ë¸ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì§ë ¬í™”/ì—­ì§ë ¬í™”ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.
* `Config.extra = 'allow'`: ëª¨ë¸ì— ì •ì˜ë˜ì§€ ì•Šì€ ì¶”ê°€ í•„ë“œê°€ ì…ë ¥ ë°ì´í„°ì— ì¡´ì¬í•˜ë”ë¼ë„ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  í—ˆìš©í•©ë‹ˆë‹¤. (Pydantic V2ì—ì„œëŠ” `model_config = ConfigDict(extra='allow')` í˜•íƒœë¡œ ì‚¬ìš©)
* ê° í•„ë“œëŠ” ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° í•­ëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
  * `text`: ì²­í¬ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©.
  * `n_char`, `n_word`, `n_line`: ë¬¸ì ìˆ˜, ë‹¨ì–´ ìˆ˜, ì¤„ ìˆ˜.
  * `e_page`, `i_page`, `i_chunk_on_page`, `n_chunk_of_page`: í˜ì´ì§€ ë‚´ì—ì„œì˜ ì²­í¬ ìœ„ì¹˜ ì •ë³´.
  * `i_chunk_on_doc`, `n_chunk_of_doc`: ë¬¸ì„œ ì „ì²´ì—ì„œì˜ ì²­í¬ ìœ„ì¹˜ ì •ë³´.
  * `n_page`: ë¬¸ì„œì˜ ì´ í˜ì´ì§€ ìˆ˜.
  * `reg_date`: ì²˜ë¦¬ ë“±ë¡ ì‹œê°„.
  * `bboxes`: í˜ì´ì§€ ë‚´ í•´ë‹¹ ì²­í¬ì˜ ê²½ê³„ ìƒì (JSON ë¬¸ìì—´ í˜•íƒœ).
  * `chunk_bboxes`: ì²­í¬ë¥¼ êµ¬ì„±í•˜ëŠ” ê° `DocItem`ì˜ ìƒì„¸ ê²½ê³„ ìƒì ì •ë³´ ë¦¬ìŠ¤íŠ¸.
  * `media_files`: ì²­í¬ ë‚´ í¬í•¨ëœ ë¯¸ë””ì–´ íŒŒì¼(ì´ë¯¸ì§€) ì •ë³´ ë¦¬ìŠ¤íŠ¸.
  * **ê³ ê°ì‚¬ë³„ í•„ë“œ**: `title`, `created_date` ë“±ì€ ê³ ê°ì‚¬ì˜ íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì¶”ê°€ëœ ë©”íƒ€ë°ì´í„° í•„ë“œì…ë‹ˆë‹¤.

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* ê³ ê°ì‚¬ì—ì„œ í•„ìš”í•œ ì¶”ê°€ì ì¸ ë©”íƒ€ë°ì´í„° í•­ëª©ì´ ìˆë‹¤ë©´, ì´ `GenOSVectorMeta` ëª¨ë¸ì— ìƒˆë¡œìš´ í•„ë“œë¥¼ ì¶”ê°€ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* í•„ë“œ íƒ€ì…ì„ ë³´ë‹¤ ì—„ê²©í•˜ê²Œ ì •ì˜í•˜ê±°ë‚˜ (ì˜ˆ: `Optional[str]`), ê¸°ë³¸ê°’ì„ ì„¤ì •í•˜ê±°ë‚˜, ìœ íš¨ì„± ê²€ì‚¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

**`GenOSVectorMetaBuilder` í´ë˜ìŠ¤ ë° ì£¼ìš” ë©”ì„œë“œ**

`GenOSVectorMeta` ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ë¹Œë” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

Python

```python
class GenOSVectorMetaBuilder:
    def __init__(self):
        """ë¹Œë” ì´ˆê¸°í™”"""
        self.text: Optional[str] = None
        self.n_char: Optional[int] = None
        # ... (ë‹¤ë¥¸ í•„ë“œë“¤ë„ ì´ˆê¸°í™”) ...
        self.title: str = None
        self.created_date: Optional[int] = None

    def set_text(self, text: str) -> "GenOSVectorMetaBuilder":
        """í…ìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ì„¤ì •"""
        self.text = text
        self.n_char = len(text)
        self.n_word = len(text.split())
        self.n_line = len(text.splitlines())
        return self

    def set_page_info(
            self, i_page: int, i_chunk_on_page: int, n_chunk_of_page: int
    ) -> "GenOSVectorMetaBuilder":
        """í˜ì´ì§€ ì •ë³´ ì„¤ì •"""
        self.i_page = i_page
        self.i_chunk_on_page = i_chunk_on_page
        self.n_chunk_of_page = n_chunk_of_page
        return self

    def set_chunk_index(self, i_chunk_on_doc: int) -> "GenOSVectorMetaBuilder":
        """ë¬¸ì„œ ì „ì²´ì˜ ì²­í¬ ì¸ë±ìŠ¤ ì„¤ì •"""
        self.i_chunk_on_doc = i_chunk_on_doc
        return self

    def set_global_metadata(self, **global_metadata) -> "GenOSVectorMetaBuilder":
        """ê¸€ë¡œë²Œ ë©”íƒ€ë°ì´í„° ë³‘í•©"""
        for key, value in global_metadata.items():
            if hasattr(self, key): # ë¹Œë” ë‚´ì— í•´ë‹¹ ì†ì„±ì´ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                setattr(self, key, value)
        return self

    def set_chunk_bboxes(self, doc_items: list, document: DoclingDocument) -> "GenOSVectorMetaBuilder":
        self.chunk_bboxes = []
        for item in doc_items:
            for prov in item.prov:
                label = item.self_ref
                type_ = item.label
                size = document.pages.get(prov.page_no).size
                page_no = prov.page_no
                bbox = prov.bbox
                bbox_data = {'l': bbox.l / size.width,
                             't': bbox.t / size.height,
                             'r': bbox.r / size.width,
                             'b': bbox.b / size.height,
                             'coord_origin': bbox.coord_origin.value}
                chunk_bboxes.append({'page': page_no, 'bbox': bbox_data, 'type': type_, 'ref': label})
        self.e_page = max([bbox['page'] for bbox in chunk_bboxes]) if chunk_bboxes else None
        self.chunk_bboxes = json.dumps(chunk_bboxes)
        return self

    def set_media_files(self, doc_items: list) -> "GenOSVectorMetaBuilder":
        temp_list = []
        for item in doc_items:
            if isinstance(item, PictureItem): # DocItemì´ PictureItemì¸ ê²½ìš°
                path = str(item.image.uri) # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
                name = path.rsplit("/", 1)[-1] # íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ
                temp_list.append({'name': name, 'type': 'image', 'ref': item.self_ref})
        self.media_files = temp_list
        return self

    def build(self) -> GenOSVectorMeta:
        """ì„¤ì •ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ìµœì¢…ì ìœ¼ë¡œ GenOSVectorMeta ê°ì²´ ìƒì„±"""
        return GenOSVectorMeta(
            text=self.text,
            n_char=self.n_char,
            # ... (ëª¨ë“  í•„ë“œë¥¼ GenOSVectorMeta ìƒì„±ìì— ì „ë‹¬) ...
            title=self.title,
            created_date=self.created_date,
        )
```

**ì„¤ëª…:**

* **`__init__`**: ë¹Œë” ë‚´ë¶€ì˜ ëª¨ë“  ì†ì„±ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ ì†ì„±ë“¤ì€ `GenOSVectorMeta`ì˜ í•„ë“œë“¤ê³¼ ëŒ€ë¶€ë¶„ ì¼ì¹˜í•©ë‹ˆë‹¤.
* **`set_text`**: ì²­í¬ì˜ í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •í•˜ê³ , ë¬¸ì ìˆ˜, ë‹¨ì–´ ìˆ˜, ì¤„ ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë‚´ë¶€ ì†ì„±ì— ì €ì¥í•©ë‹ˆë‹¤.
* **`set_page_info`**: í˜ì´ì§€ ë²ˆí˜¸, í˜ì´ì§€ ë‚´ ì²­í¬ ì¸ë±ìŠ¤, í˜ì´ì§€ ë‚´ ì´ ì²­í¬ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
* **`set_chunk_index`**: ë¬¸ì„œ ì „ì²´ì—ì„œì˜ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
* **`set_global_metadata`**: `DocumentProcessor.compose_vectors`ì—ì„œ ì „ë‹¬ë°›ì€ `global_metadata` ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ì„ ë¹Œë”ì˜ í•´ë‹¹ ì†ì„±ì— í• ë‹¹í•©ë‹ˆë‹¤. ë¹Œë” ë‚´ì— `global_metadata`ì˜ í‚¤ì™€ ë™ì¼í•œ ì´ë¦„ì˜ ì†ì„±ì´ ìˆì–´ì•¼ ê°’ì´ í• ë‹¹ë©ë‹ˆë‹¤.
* **`set_chunk_bboxes`**: ì²­í¬ë¥¼ êµ¬ì„±í•˜ëŠ” ëª¨ë“  `DocItem`ë“¤ì˜ ìƒì„¸í•œ ê²½ê³„ ìƒì ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ê° í•­ëª©ì€ í˜ì´ì§€ ë²ˆí˜¸, ì •ê·œí™”ëœ ì¢Œí‘œ(0\~1 ê°’), `DocItem`ì˜ íƒ€ì… ë° ì°¸ì¡° IDë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì •ê·œí™”ëœ ì¢Œí‘œëŠ” í˜ì´ì§€ í¬ê¸°ì— ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ë¯€ë¡œ, ë‹¤ì–‘í•œ í¬ê¸°ì˜ í˜ì´ì§€ì—ì„œë„ ì¼ê´€ë˜ê²Œ ìœ„ì¹˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **`set_media_files`**: ì²­í¬ ë‚´ì— `PictureItem`(ì´ë¯¸ì§€)ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, í•´ë‹¹ ì´ë¯¸ì§€ì˜ íŒŒì¼ ì´ë¦„, íƒ€ì…("image"), ì°¸ì¡° IDë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
* **`build`**: ì§€ê¸ˆê¹Œì§€ `set_...` ë©”ì„œë“œë“¤ì„ í†µí•´ ë¹Œë” ë‚´ë¶€ì— ì¶•ì ëœ ëª¨ë“  ì†ì„±ê°’ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ `GenOSVectorMeta` Pydantic ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* `GenOSVectorMeta` ëª¨ë¸ì— ìƒˆë¡œìš´ í•„ë“œë¥¼ ì¶”ê°€í–ˆë‹¤ë©´, ì´ ë¹Œë”ì—ë„ í•´ë‹¹ í•„ë“œë¥¼ ìœ„í•œ ë‚´ë¶€ ì†ì„±ê³¼ `set_...` ë©”ì„œë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
* `build` ë©”ì„œë“œì—ì„œ `GenOSVectorMeta` ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë„ ì¸ìë¡œ ì „ë‹¬í•˜ë„ë¡ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
* íŠ¹ì • í•„ë“œê°’ì„ ì„¤ì •í•˜ê¸° ì „ì— ì¶”ê°€ì ì¸ ê°€ê³µ ë¡œì§(ì˜ˆ: ë‚ ì§œ í˜•ì‹ ë³€í™˜, íŠ¹ì • ì½”ë“œê°’ ë§¤í•‘ ë“±)ì´ í•„ìš”í•˜ë‹¤ë©´ í•´ë‹¹ `set_...` ë©”ì„œë“œ ë‚´ë¶€ì— êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

### ğŸ“‚ ê³µí†µ ì „ì²˜ë¦¬ íë¦„

#### `DocumentProcessor`

`DocumentProcessor` í´ë˜ìŠ¤ëŠ” Genos ì˜ ì „ì²˜ë¦¬ê¸°ê°€ í˜¸ì¶œë˜ëŠ” ê´€ë¬¸ì…ë‹ˆë‹¤. ë‚´ë¶€ êµ¬ì„±ì„ ë³´ë©´, ë¬¸ì„œë¥¼ ë¡œë“œ, ë³€í™˜, ë¶„í• í•˜ê³  ê° ë¶€ë¶„ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

**`__init__` (ì´ˆê¸°í™”)**

`DocumentProcessor` ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë  ë•Œ í˜¸ì¶œë˜ëŠ” ì´ˆê¸°í™” ë©”ì„œë“œì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì£¼ìš” ì„¤ì •ë“¤ì´ ì •ì˜ë©ë‹ˆë‹¤.

Python

```python
class DocumentProcessor:

    def __init__(self):
        '''
        initialize Document Converter
        '''
        ocr_options = PaddleOcrOptions(
            force_full_page_ocr=False,
            lang=['korean'],
            text_score=0.3)

        self.page_chunk_counts = defaultdict(int)
        device = AcceleratorDevice.AUTO
        num_threads = 8
        accelerator_options = AcceleratorOptions(num_threads=num_threads, device=device)
        pipe_line_options = PdfPipelineOptions()
        pipe_line_options.generate_page_images = True
        pipe_line_options.generate_picture_images = True
        pipe_line_options.do_ocr = False
        pipe_line_options.artifacts_path = Path("/models/")
        pipe_line_options.do_table_structure = True
        pipe_line_options.images_scale = 2
        pipe_line_options.table_structure_options.do_cell_matching = True
        pipe_line_options.table_structure_options.mode = TableFormerMode.ACCURATE
        pipe_line_options.accelerator_options = accelerator_options

        # Simple íŒŒì´í”„ë¼ì¸ ì˜µì…˜ì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
        self.simple_pipeline_options = PipelineOptions()
        self.simple_pipeline_options.save_images = False

        # ocr íŒŒì´í”„ë¼ì¸ ì˜µì…˜
        self.ocr_pipe_line_options = PdfPipelineOptions()
        self.ocr_pipe_line_options = self.pipe_line_options.model_copy(deep=True)
        self.ocr_pipe_line_options.do_ocr = True
        self.ocr_pipe_line_options.ocr_options = ocr_options.model_copy(deep=True)
        self.ocr_pipe_line_options.ocr_options.force_full_page_ocr = True

        # ê¸°ë³¸ ì»¨ë²„í„°ë“¤ ìƒì„±
        self._create_converters()

        # enrichment ì˜µì…˜ ì„¤ì •
        self.enrichment_options = DataEnrichmentOptions(
            do_toc_enrichment=True,
            toc_doc_type="law",
            extract_metadata=True,
            toc_api_provider="custom",
            toc_api_base_url="http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions",
            metadata_api_base_url="http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions",
            toc_api_key="9e32423947fd4a5da07a28962fe88487",
            metadata_api_key="9e32423947fd4a5da07a28962fe88487",
            toc_model="/model/snapshots/9eb2daaa8597bf192a8b0e73f848f3a102794df5",
            metadata_model="/model/snapshots/9eb2daaa8597bf192a8b0e73f848f3a102794df5",
            toc_temperature=0.0,
            toc_top_p=0,
            toc_seed=33,
            toc_max_tokens=10000
        )
```

**ì„¤ëª…:**

* `PaddleOcrOptions`: OCR(ê´‘í•™ ë¬¸ì ì¸ì‹) ìˆ˜í–‰ê³¼ ê´€ë ¨ëœ ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
  * `force_full_page_ocr = False`: OCRì„ ë¬´ì¡°ê±´ ìˆ˜í–‰í• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤.
  * `lang = ["korean"]`: OCR ìˆ˜í–‰ ì‹œ ì¸ì‹í•  ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (ì˜ˆ: í•œêµ­ì–´) ì •í™•í•œ ì–¸ì–´ ì„¤ì •ì€ OCR ì„±ëŠ¥ì— ì¤‘ìš”í•©ë‹ˆë‹¤.
  * `text_score=0.3`: OCR ìˆ˜í–‰í›„ text ê²°ê³¼ì˜ ì ìˆ˜ê°€ `text_score` ë³´ë‹¤ ë†’ì€ textë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. text ì ìˆ˜ëŠ” 0.0 ~ 1.0 ë²”ìœ„ë¡œ ë¶€ì—¬ë˜ë©° ë†’ì„ ìˆ˜ë¡ ì‹ ë¢°ê°€ ë†’ë‹¤ëŠ” ì˜ë¯¸ë¥¼ ê°–ìŠµë‹ˆë‹¤.
* `self.page_chunk_counts = defaultdict(int)`: í˜ì´ì§€ë³„ë¡œ ìƒì„±ëœ ì²­í¬ì˜ ìˆ˜ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
* `AcceleratorOptions`: CPU ìŠ¤ë ˆë“œ ìˆ˜(`num_threads`), ì‚¬ìš©í•  ì¥ì¹˜(`device`, ì˜ˆ: CPU/GPU ìë™ ì„ íƒ) ë“± í•˜ë“œì›¨ì–´ ê°€ì† ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
* **`PdfPipelineOptions`**: PDF ì²˜ë¦¬ ë°©ì‹ì— ëŒ€í•œ ìƒì„¸ ì„¤ì •ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ë¬¸ì„œ ìœ í˜• ë° ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì´ ë¶€ë¶„ì„ ì£¼ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ê²Œ ë©ë‹ˆë‹¤.
  * `generate_page_images = True`: PDF ê° í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ìƒì„±í• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤.
  * `generate_picture_images = True`: PDF ë‚´ì— ì‚½ì…ëœ ê·¸ë¦¼ë“¤ì„ ë³„ë„ì˜ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì¶”ì¶œí• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤.
  * `do_ocr = False`: OCR(ê´‘í•™ ë¬¸ì ì¸ì‹) ìˆ˜í–‰ ì—¬ë¶€ì…ë‹ˆë‹¤. ìŠ¤ìº”ëœ PDFì™€ ê°™ì´ í…ìŠ¤íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ì´ë¯¸ì§€ì„± PDFì˜ ê²½ìš° `True`ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
  * `artifacts_path`: ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ ë˜ëŠ” ì„ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  * `do_table_structure = True`: í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€ì…ë‹ˆë‹¤.
  * `table_structure_options.mode = TableFormerMode.ACCURATE`: í…Œì´ë¸” ì¸ì‹ ì •í™•ë„ ëª¨ë“œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. `ACCURATE`ëŠ” ì •í™•ë„ë¥¼ ìš°ì„ í•˜ë©°, `FAST`ëŠ” ì†ë„ë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.
* **`ocr_pipe_line_options`**: PDF ë‚´ì˜ í…ìŠ¤íŠ¸ì— ëŒ€í•œ OCR ìˆ˜í–‰ì„ ìœ„í•´ì„œ ì‚¬ìš©ë˜ëŠ” ì„¤ì •ì…ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ `pipe_line_options` ì˜µì…˜ì„ ë³µì‚¬í•´ì„œ ì ìš©í•˜ë©°, OCR ìˆ˜í–‰ì— ëŒ€í•œ ì˜µì…˜ì„ ì¶”ê°€ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
  * `do_ocr = True`: OCR(ê´‘í•™ ë¬¸ì ì¸ì‹) ìˆ˜í–‰ ì—¬ë¶€ì…ë‹ˆë‹¤. ìŠ¤ìº”ëœ PDFì™€ ê°™ì´ í…ìŠ¤íŠ¸ ì •ë³´ê°€ ì—†ëŠ” ì´ë¯¸ì§€ì„± PDFì˜ ê²½ìš° `True`ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
  * `force_full_page_ocr = True`: ë¬´ì¡°ê±´ OCRì„ ìˆ˜í–‰í•˜ëŠ” ì„¤ì •ì…ë‹ˆë‹¤. PDFì— text ì •ë³´ê°€ ì¡´ì¬í•˜ë”ë¼ë„ ë¬´ì‹œí•˜ê³  OCR ìˆ˜í–‰ì„ í•©ë‹ˆë‹¤.
* **`enrichment_options`**: ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë³´ê°•í•˜ê¸° ìœ„í•œ ì„¤ì •ì…ë‹ˆë‹¤. ì´ ì˜µì…˜ì„ í†µí•´ ë¬¸ì„œì˜ ëª©ì°¨, ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  * `do_toc_enrichment = True`: ëª©ì°¨ ë³´ê°• ê¸°ëŠ¥ ì‚¬ìš© ì—¬ë¶€ì…ë‹ˆë‹¤.
  * `toc_doc_type = "law"`: ëª©ì°¨ ìƒì„± ì‹œ ë¬¸ì„œ ìœ í˜•ì„ ì§€ì •í•©ë‹ˆë‹¤. ê·œì •ë¬¸ì„œëŠ” "law"ë¡œ ì„¤ì •ë˜ë©°, ì¼ë°˜ë¬¸ì„œëŠ” "normal"ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.
  * `extract_metadata = True`: ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí• ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤. ì‘ì„±ì¼ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  * `toc_api_provider = "custom"`: ëª©ì°¨ API ì œê³µì ì„¤ì •ì…ë‹ˆë‹¤.
  * `toc_api_base_url = "http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions"`: ëª©ì°¨ API ê¸°ë³¸ URLì…ë‹ˆë‹¤.
  * `metadata_api_base_url = "http://llmops-gateway-api-service:8080/serving/13/23/v1/chat/completions"`: ë©”íƒ€ë°ì´í„° API ê¸°ë³¸ URLì…ë‹ˆë‹¤.
  * `toc_api_key = "9e32423947fd4a5da07a28962fe88487"`: ëª©ì°¨ API í‚¤ì…ë‹ˆë‹¤.
  * `metadata_api_key = "9e32423947fd4a5da07a28962fe88487"`: ë©”íƒ€ë°ì´í„° API í‚¤ì…ë‹ˆë‹¤.
  * `toc_model = "/model/"`: ëª©ì°¨ ëª¨ë¸ ê²½ë¡œì…ë‹ˆë‹¤.
  * `metadata_model = "/model/"`: ë©”íƒ€ë°ì´í„° ëª¨ë¸ ê²½ë¡œì…ë‹ˆë‹¤.
  * `toc_temperature = 0.0`: ëª©ì°¨ ìƒì„± ì‹œ ì˜¨ë„ ì„¤ì •ì…ë‹ˆë‹¤.
  * `toc_top_p = 0`: ëª©ì°¨ ìƒì„± ì‹œ top-p ì„¤ì •ì…ë‹ˆë‹¤.
  * `toc_seed = 33`: ëª©ì°¨ ìƒì„± ì‹œ ì‹œë“œ ì„¤ì •ì…ë‹ˆë‹¤.
  * `toc_max_tokens = 10000`: ëª©ì°¨ ìƒì„± ì‹œ ìµœëŒ€ í† í° ìˆ˜ ì„¤ì •ì…ë‹ˆë‹¤. ê·œì •ë¬¸ì„œì˜ ëª©ì°¨ëŠ” 10000ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¶©ë¶„í•œ ê¸¸ì´ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.

***

**`_create_converters`**

ë¬¸ì„œ ë³€í™˜ì— ì‚¬ìš©í•  ë³€í™˜ê¸°ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

Python

```python
    def _create_converters(self):
        """ì»¨ë²„í„°ë“¤ì„ ìƒì„±í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        self.converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=self.pipe_line_options,
                        backend=DoclingParseV4DocumentBackend
                    ),
                }
            )
        self.second_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=self.pipe_line_options,
                    backend=PyPdfiumDocumentBackend
                ),
            },
        )
        self.ocr_converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=self.ocr_pipe_line_options,
                        backend=DoclingParseV4DocumentBackend
                    ),
                }
            )
        self.ocr_second_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=self.ocr_pipe_line_options,
                    backend=PyPdfiumDocumentBackend
                ),
            },
        )
```

**ì„¤ëª…:**

* **`self.converter` (ê¸°ë³¸ ë³€í™˜ê¸°)**: PDFë¥¼ ì²˜ë¦¬í•˜ëŠ” Primary ë¬¸ì„œ ë³€í™˜ê¸°ì…ë‹ˆë‹¤. ì´ ë°±ì—”ë“œëŠ” ë³µì¡í•œ ë ˆì´ì•„ì›ƒì´ë‚˜ í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ì— ê°•ì ì´ ìˆìŠµë‹ˆë‹¤.
* **`self.second_converter` (ë³´ì¡° ë³€í™˜ê¸°)**: `PyPdfiumDocumentBackend`ë¥¼ ì‚¬ìš©í•˜ëŠ” ë³´ì¡° ë³€í™˜ê¸°ì…ë‹ˆë‹¤. ê¸°ë³¸ ë³€í™˜ê¸°ê°€ íŠ¹ì • PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¬ ê²½ìš°, ì´ ë³€í™˜ê¸°ë¥¼ í†µí•´ ì¬ì‹œë„í•˜ëŠ” í´ë°±(fallback) ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. `PyPdfium`ì€ ë¹„êµì  ê°„ë‹¨í•œ PDFë‚˜ íŠ¹ì • ìœ í˜•ì˜ PDF ì²˜ë¦¬ì— ë” ì•ˆì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **`self.ocr_converter` ë° `self.ocr_second_converter`**: OCRì´ í•„ìš”í•œ ë¬¸ì„œ(ì˜ˆ: ìŠ¤ìº”ëœ ì´ë¯¸ì§€ PDF)ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë³€í™˜ê¸°ë“¤ì…ë‹ˆë‹¤. ì´ë“¤ì€ ê°ê° `DoclingParseV4DocumentBackend`ì™€ `PyPdfiumDocumentBackend`ë¥¼ ì‚¬ìš©í•˜ì—¬ OCR ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. OCRì´ í•„ìš”í•œ ê²½ìš°, ì´ ë³€í™˜ê¸°ë“¤ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.s

***

**`load_documents_with_docling`, `load_documents_with_docling_ocr` ë° `load_documents`**

ë¬¸ì„œë¥¼ ì‹¤ì œ ë¡œë“œí•˜ê³  íŒŒì‹±í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.

Python

```python
    def load_documents_with_docling(self, file_path: str, **kwargs: dict) -> DoclingDocument:
        # kwargsì—ì„œ save_images ê°’ì„ ê°€ì ¸ì™€ì„œ ì˜µì…˜ ì—…ë°ì´íŠ¸
        save_images = kwargs.get('save_images', True)
        include_wmf = kwargs.get('include_wmf', False)

        # save_images ì˜µì…˜ì´ í˜„ì¬ ì„¤ì •ê³¼ ë‹¤ë¥´ë©´ ì»¨ë²„í„° ì¬ìƒì„±
        if (self.simple_pipeline_options.save_images != save_images or
            getattr(self.simple_pipeline_options, 'include_wmf', False) != include_wmf):
            self.simple_pipeline_options.save_images = save_images
            self.simple_pipeline_options.include_wmf = include_wmf
            self._create_converters()

        try:
            conv_result: ConversionResult = self.converter.convert(file_path, raises_on_error=True)
        except Exception as e:
            conv_result: ConversionResult = self.second_converter.convert(file_path, raises_on_error=True)
        return conv_result.document

    def load_documents_with_docling_ocr(self, file_path: str, **kwargs: dict) -> DoclingDocument:
        # kwargsì—ì„œ save_images ê°’ì„ ê°€ì ¸ì™€ì„œ ì˜µì…˜ ì—…ë°ì´íŠ¸
        save_images = kwargs.get('save_images', True)
        include_wmf = kwargs.get('include_wmf', False)

        # save_images ì˜µì…˜ì´ í˜„ì¬ ì„¤ì •ê³¼ ë‹¤ë¥´ë©´ ì»¨ë²„í„° ì¬ìƒì„±
        if (self.simple_pipeline_options.save_images != save_images or
            getattr(self.simple_pipeline_options, 'include_wmf', False) != include_wmf):
            self.simple_pipeline_options.save_images = save_images
            self.simple_pipeline_options.include_wmf = include_wmf
            self._create_converters()

        try:
            conv_result: ConversionResult = self.ocr_converter.convert(file_path, raises_on_error=True)
        except Exception as e:
            conv_result: ConversionResult = self.ocr_second_converter.convert(file_path, raises_on_error=True)
        return conv_result.document

    def load_documents(self, file_path: str, **kwargs: dict) -> DoclingDocument:
        # ducling ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ ë¡œë“œ
        return self.load_documents_with_docling(file_path, **kwargs)
        # return documents
```

**ì„¤ëª…:**

* `load_documents_with_docling(self, file_path: str, **kwargs: dict) -> DoclingDocument`:
  * ì£¼ì–´ì§„ `file_path`ë¡œë¶€í„° ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  `DoclingDocument` ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
  * í•µì‹¬ ë¡œì§ì€ `try-except` ë¸”ë¡ ì•ˆì— ìˆìŠµë‹ˆë‹¤.
    * ë¨¼ì € `self.converter` (ê¸°ë³¸ ë³€í™˜ê¸°: `DoclingParseV4DocumentBackend`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤.
    * ë§Œì•½ `Exception`ì´ ë°œìƒí•˜ë©´ (ì¦‰, ê¸°ë³¸ ë³€í™˜ê¸°ê°€ ì‹¤íŒ¨í•˜ë©´), `self.second_converter` (ë³´ì¡° ë³€í™˜ê¸°: `PyPdfiumDocumentBackend`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì„œ ì²˜ë¦¬ì˜ ì•ˆì •ì„±ì„ ë†’ì—¬ì¤ë‹ˆë‹¤.
  * `raises_on_error=True`ëŠ” ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ë„ë¡ í•©ë‹ˆë‹¤.
  * ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ëœ `conv_result.document` (ì¦‰, `DoclingDocument` ê°ì²´)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* `load_documents_with_docling_ocr(self, file_path: str, **kwargs: dict) -> DoclingDocument`:
  * ì£¼ì–´ì§„ `file_path`ë¡œë¶€í„° OCRì´ í•„ìš”í•œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  `DoclingDocument` ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
  * í•µì‹¬ ë¡œì§ì€ `try-except` ë¸”ë¡ ì•ˆì— ìˆìŠµë‹ˆë‹¤.
    * ë¨¼ì € `self.ocr_converter` (ê¸°ë³¸ OCR ë³€í™˜ê¸°: `DoclingParseV4DocumentBackend`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤.
    * ë§Œì•½ `Exception`ì´ ë°œìƒí•˜ë©´ (ì¦‰, ê¸°ë³¸ OCR ë³€í™˜ê¸°ê°€ ì‹¤íŒ¨í•˜ë©´), `self.ocr_second_converter` (ë³´ì¡° OCR ë³€í™˜ê¸°: `PyPdfiumDocumentBackend`)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤. ì´ëŠ” ë¬¸ì„œ ì²˜ë¦¬ì˜ ì•ˆì •ì„±ì„ ë†’ì—¬ì¤ë‹ˆë‹¤.s
* `load_documents(self, file_path: str, **kwargs: dict) -> DoclingDocument`:
  * `load_documents_with_docling` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ê³µê°œ ì¸í„°í˜ì´ìŠ¤ ì—­í• ì„ í•©ë‹ˆë‹¤.
  * `**kwargs`ë¥¼ í†µí•´ ì¶”ê°€ì ì¸ íŒŒë¼ë¯¸í„°ë¥¼ ë‚´ë¶€ ë©”ì„œë“œë¡œ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. (í˜„ì¬,`kwargs`ê°€ ì§ì ‘ì ìœ¼ë¡œ í™œìš©ë˜ì§€ëŠ” ì•Šê³  ìˆìŠµë‹ˆë‹¤.)

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* ë¬¸ì„œ ë¡œë”© ì „ íŠ¹ì • ì „ì²˜ë¦¬ ì‘ì—…ì´ í•„ìš”í•˜ê±°ë‚˜, PDF ì™¸ ë‹¤ë¥¸ í¬ë§·ì— ëŒ€í•´ ë³„ë„ì˜ ë¡œì§ì„ ì ìš©í•˜ê³  ì‹¶ë‹¤ë©´ ì´ ë¶€ë¶„ì„ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* `kwargs`ë¥¼ í™œìš©í•˜ì—¬ `PdfPipelineOptions`ì˜ ì¼ë¶€ ê°’ì„ ë™ì ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: íŠ¹ì • ë¬¸ì„œ ìœ í˜•ì— ë”°ë¼ OCR í™œì„±í™”).

***

**`split_documents`**

ë¡œë“œëœ ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ì‘ì€ ë‹¨ìœ„(ì²­í¬)ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

Python

```python
    def split_documents(self, documents: DoclingDocument, **kwargs: dict) -> list[DocChunk]:
        chunker: HybridChunker = HybridChunker(
            max_tokens=1000,
            merge_peers=True
        )

        chunks: List[DocChunk] = list(chunker.chunk(dl_doc=documents, **kwargs))
        for chunk in chunks:
            self.page_chunk_counts[chunk.meta.doc_items[0].prov[0].page_no] += 1
        return chunks
```

**ì„¤ëª…:**

* `chunker: HybridChunker = HybridChunker()`: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•´ `HybridChunker` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. `HybridChunker`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ê³„ì¸µì  êµ¬ì¡°(Hierarchical)ì™€ ì˜ë¯¸ë¡ ì  ë¶„í• (Semantic, ì£¼ì„ ì²˜ë¦¬ëœ `semchunk` ì˜ì¡´ì„± ë¶€ë¶„ì—ì„œ ìœ ì¶”)ì„ ê²°í•©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.&#x20;
  * `max_tokens`ëŠ” ê° ì²­í¬ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
  * `merge_peers`ëŠ” ì¸ì ‘í•œ ì²­í¬ë“¤ì„ ë³‘í•©í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
* `chunks: List[DocChunk] = list(chunker.chunk(dl_doc=documents, **kwargs))`: `chunker`ì˜ `chunk` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ `DoclingDocument`ë¥¼ `DocChunk` ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. `**kwargs`ëŠ” ì²­í‚¹ ê³¼ì •ì— í•„ìš”í•œ ì¶”ê°€ ì˜µì…˜ì„ ì „ë‹¬í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ .
* `for chunk in chunks: self.page_chunk_counts[chunk.meta.doc_items[0].prov[0].page_no] += 1`: ê° ì²­í¬ê°€ ì–´ë–¤ í˜ì´ì§€ì—ì„œ ì™”ëŠ”ì§€ íŒŒì•…í•˜ì—¬ `self.page_chunk_counts` ë”•ì…”ë„ˆë¦¬ì— í˜ì´ì§€ë³„ ì²­í¬ ìˆ˜ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. ì´ëŠ” ì¶”í›„ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œ í™œìš©ë©ë‹ˆë‹¤. (`chunk.meta.doc_items[0].prov[0].page_no`ëŠ” ì²­í¬ë¥¼ êµ¬ì„±í•˜ëŠ” ì²«ë²ˆì§¸ ë¬¸ì„œ ì•„ì´í…œì˜ ì²«ë²ˆì§¸ ì¶œì²˜ ì •ë³´ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.)

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* `HybridChunker`ì˜ ì„¤ì • (ì˜ˆ: ìµœëŒ€ í† í° ìˆ˜ `max_tokens`, ë³‘í•© ì˜µì…˜ `merge_peers`)ì€ `HybridChunker` í´ë˜ìŠ¤ ì •ì˜ ë¶€ë¶„ì—ì„œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

**`parse_created_date`**

ì‘ì„±ì¼ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

Python

```python
    def parse_created_date(self, date_text: str) -> Optional[int]:
        """
        ì‘ì„±ì¼ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ë¡œ ë³€í™˜

        Args:
            date_text: ì‘ì„±ì¼ í…ìŠ¤íŠ¸ (YYYY-MM ë˜ëŠ” YYYY-MM-DD í˜•ì‹)

        Returns:
            YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜, íŒŒì‹± ì‹¤íŒ¨ì‹œ None
        """
        if not date_text or not isinstance(date_text, str) or date_text == "None":
            return 0

        # ê³µë°± ì œê±° ë° ì •ë¦¬
        date_text = date_text.strip()

        # YYYY-MM-DD í˜•ì‹ ë§¤ì¹­
        match_full = re.match(r'^(\d{4})-(\d{1,2})-(\d{1,2})$', date_text)
        if match_full:
            year, month, day = match_full.groups()
            try:
                # ìœ íš¨í•œ ë‚ ì§œì¸ì§€ ê²€ì¦
                datetime(int(year), int(month), int(day))
                return int(f"{year}{month.zfill(2)}{day.zfill(2)}")
            except ValueError:
                pass

        # YYYY-MM í˜•ì‹ ë§¤ì¹­ (ì¼ìëŠ” 01ë¡œ ì„¤ì •)
        match_month = re.match(r'^(\d{4})-(\d{1,2})$', date_text)
        if match_month:
            year, month = match_month.groups()
            try:
                # ìœ íš¨í•œ ì›”ì¸ì§€ ê²€ì¦
                datetime(int(year), int(month), 1)
                return int(f"{year}{month.zfill(2)}01")
            except ValueError:
                pass

        # YYYY í˜•ì‹ ë§¤ì¹­ (ì›”ì¼ì€ 0101ë¡œ ì„¤ì •)
        match_year = re.match(r'^(\d{4})$', date_text)
        if match_year:
            year = match_year.group(1)
            try:
                datetime(int(year), 1, 1)
                return int(f"{year}0101")
            except ValueError:
                pass

        return 0
```

**ì„¤ëª…:**

* `date_text`ê°€ ìœ íš¨í•œ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. ë¹„ì–´ìˆê±°ë‚˜ "None"ì¸ ê²½ìš° 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
* ê³µë°±ì„ ì œê±°í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤.
* `YYYY-MM-DD` í˜•ì‹ê³¼ `YYYY-MM` í˜•ì‹, `YYYY` í˜•ì‹ì— ëŒ€í•´ ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ì¹­ì„ ì‹œë„í•©ë‹ˆë‹¤.
* ê° í˜•ì‹ì— ëŒ€í•´ ìœ íš¨í•œ ë‚ ì§œì¸ì§€ ê²€ì¦í•˜ê³ , YYYYMMDD í˜•ì‹ì˜ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
* ëª¨ë“  ë§¤ì¹­ì´ ì‹¤íŒ¨í•œ ê²½ìš° 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
* ì‘ì„±ì¼ì´ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì¸ ê²½ìš° 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„° ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

***

**`enrichment`**

ë¬¸ì„œì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ì •ë³´ë¥¼ ë³´ê°•í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Python

```python
    def enrichment(self, document: DoclingDocument, **kwargs: dict) -> DoclingDocument:

        # ìƒˆë¡œìš´ enriched result ë°›ê¸°
        document = enrich_document(document, self.enrichment_options)
        return document
```

**ì„¤ëª…:**

* ë¬¸ì„œì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ì •ë³´ë¥¼ ë³´ê°•í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* `enrich_document` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ë³´ê°•í•˜ê³ , ë³´ê°•ëœ ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* ë¬¸ì„œ ë³´ê°•ì— í•„ìš”í•œ ì¶”ê°€ ì˜µì…˜ì€ `self.enrichment_options`ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.

***

**`compose_vectors`**

ë¶„í• ëœ ì²­í¬ë“¤ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ìµœì¢…ì ì¸ ë²¡í„°(ë”•ì…”ë„ˆë¦¬ í˜•íƒœ) ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì´ ê³ ê°ì‚¬ë³„ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ëŠ” ë¶€ë¶„ì´ë©° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

Python

```python
    async def compose_vectors(self, document: DoclingDocument, chunks: List[DocChunk], file_path: str, request: Request,
                              **kwargs: dict) -> \
            list[dict]:
        title = ""
        created_date = 0
        try:
            if (document.key_value_items and
                    len(document.key_value_items) > 0 and
                    hasattr(document.key_value_items[0], 'graph') and
                    hasattr(document.key_value_items[0].graph, 'cells') and
                    len(document.key_value_items[0].graph.cells) > 1):
                # ì‘ì„±ì¼ ì¶”ì¶œ (cells[1])
                date_text = document.key_value_items[0].graph.cells[1].text
                created_date = self.parse_created_date(date_text)
        except (AttributeError, IndexError) as e:
            pass

        for item, _ in document.iterate_items():
            if hasattr(item, 'label'):
                if item.label == DocItemLabel.TITLE:
                    title = item.text.strip() if item.text else ""
                    break
        global_metadata = dict(
            n_chunk_of_doc=len(chunks),
            n_page=document.num_pages(),
            reg_date=datetime.now().isoformat(timespec='seconds') + 'Z',
            created_date=created_date,
            title=title
        )

        current_page = None
        chunk_index_on_page = 0
        vectors = []
        upload_tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            chunk_page = chunk.meta.doc_items[0].prov[0].page_no
            content = self.safe_join(chunk.meta.headings) + chunk.text

            if chunk_page != current_page:
                current_page = chunk_page
                chunk_index_on_page = 0

            vector = (GenOSVectorMetaBuilder()
                      .set_text(content)
                      .set_page_info(chunk_page, chunk_index_on_page, self.page_chunk_counts[chunk_page])
                      .set_chunk_index(chunk_idx)
                      .set_global_metadata(**global_metadata)
                      .set_chunk_bboxes(chunk.meta.doc_items, document)
                      .set_media_files(chunk.meta.doc_items)
                      ).build()
            vectors.append(vector)

            chunk_index_on_page += 1
            file_list = self.get_media_files(chunk.meta.doc_items)
            upload_tasks.append(asyncio.create_task(
                upload_files(file_list, request=request)
            ))

        if upload_tasks:
            await asyncio.gather(*upload_tasks)

        return vectors
```

**ì„¤ëª…:**

**í•œêµ­ì€í–‰ ê¸°ë¡ë¬¼ì˜ ë©”íƒ€ë°ì´íƒ€ Mapping ì˜ˆ**

* **`global_metadata`**: ë¬¸ì„œ ì „ì²´ì— ê³µí†µì ìœ¼ë¡œ ì ìš©ë  ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
  * `n_chunk_of_doc=len(chunks)`: ë¬¸ì„œ ë‚´ ì´ ì²­í¬ ìˆ˜.
  * `n_page=document.num_pages()`: ë¬¸ì„œì˜ ì´ í˜ì´ì§€ ìˆ˜.
  * `reg_date`: í˜„ì¬ ì‹œê°„ì„ ISO í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë“±ë¡ì¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
  * **ê³ ê°ì‚¬ë³„ í•„ë“œ**: `created_date`, `title` ë“±ì€ enrichmentë¥¼ í†µí•´ ê°’ì„ ê°€ì ¸ì™€ ì„¤ì •ë©ë‹ˆë‹¤. **ì´ ë¶€ë¶„ì´ ê³ ê°ì‚¬ ì‹œìŠ¤í…œê³¼ ì—°ë™í•˜ì—¬ ë¬¸ì„œì˜ ê³ ìœ  ë©”íƒ€ì •ë³´ë¥¼ ì£¼ì…í•˜ëŠ” í•µì‹¬ ì§€ì ì…ë‹ˆë‹¤.**
* ë£¨í”„ (`for chunk_idx, chunk in enumerate(chunks):`): ê° ì²­í¬ë¥¼ ìˆœíšŒí•˜ë©° ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  * `chunk_page = chunk.meta.doc_items[0].prov[0].page_no`: í˜„ì¬ ì²­í¬ì˜ ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
  * `content = self.safe_join(chunk.meta.headings) + chunk.text`: ì²­í¬ì˜ ì œëª©(headings)ë“¤ê³¼ ì‹¤ì œ í…ìŠ¤íŠ¸(text)ë¥¼ ê²°í•©í•˜ì—¬ ì²­í¬ì˜ ì „ì²´ ë‚´ìš©ì„ êµ¬ì„±í•©ë‹ˆë‹¤. `safe_join`ì€ ì œëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¡œ ë³´ì…ë‹ˆë‹¤.
  * **`GenOSVectorMetaBuilder()`**: `GenOSVectorMetaBuilder`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì´ë‹ ë°©ì‹ìœ¼ë¡œ ê° ë©”íƒ€ë°ì´í„° í•„ë“œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤ (ìƒì„¸ ë‚´ìš©ì€ `GenOSVectorMetaBuilder` ì„¹ì…˜ ì°¸ì¡°).
    * `.set_text(content)`: ì²­í¬ ë‚´ìš© ì„¤ì •.
    * `.set_page_info(...)`: í˜ì´ì§€ ê´€ë ¨ ì •ë³´ ì„¤ì •.
    * `.set_chunk_index(chunk_idx)`: ë¬¸ì„œ ë‚´ ì²­í¬ ì¸ë±ìŠ¤ ì„¤ì •.
    * `.set_global_metadata(**global_metadata)`: ìœ„ì—ì„œ ì •ì˜í•œ `global_metadata`ë¥¼ ì „ë‹¬.
    * `.set_chunk_bboxes(...)`: ì²­í¬ë¥¼ êµ¬ì„±í•˜ëŠ” ì„¸ë¶€ í•­ëª©ë“¤ì˜ ê²½ê³„ ìƒì ì •ë³´ ì„¤ì •.
    * `.set_media_files(...)`: ì²­í¬ ë‚´ ì´ë¯¸ì§€ íŒŒì¼ ì •ë³´ ì„¤ì •.
  * `.build()`: ì„¤ì •ëœ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ `GenOSVectorMeta` ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  * `vectors.append(...)`: ìƒì„±ëœ `GenOSVectorMeta` Pydantic ëª¨ë¸ ê°ì²´ë¥¼ `vectors` ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
* í˜ì´ì§€ ë³€ê²½ ê°ì§€ ë¡œì§: `current_page`ì™€ `chunk_index_on_page`ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ê°€ ë°”ë€” ë•Œë§ˆë‹¤ í˜ì´ì§€ ë‚´ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸ (ë§¤ìš° ì¤‘ìš”):**

* **`global_metadata` í™•ì¥**: ê³ ê°ì‚¬ì˜ ê³ ìœ í•œ ë¬¸ì„œ ì†ì„±ë“¤(ì˜ˆ: ì‘ì„±ì¼, ë¬¸ì„œì œëª© ë“±)ì„ `global_metadata`ì— ì¶”ê°€í•˜ê³ , ìµœì¢… ë©”íƒ€ë°ì´í„°ì— í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* `content` êµ¬ì„± ë°©ì‹ ë³€ê²½: ë‹¨ìˆœíˆ ì œëª©ê³¼ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹˜ëŠ” ê²ƒ ì™¸ì—, íŠ¹ì • ìˆœì„œë¡œ ì¬ë°°ì—´í•˜ê±°ë‚˜ ìš”ì•½ ì •ë³´ë¥¼ ì¶”ê°€í•˜ëŠ” ë“±ì˜ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

**`check_glyph_text`, `check_glyphs`**

PDFíŒŒì¼ì— GLYPH í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤. GLYPHëŠ” PDFì¸ì½”ë”©ì´ ê¹¨ì§„ ê²½ìš°ì— ë‚˜íƒ€ë‚˜ëŠ” í•­ëª©ì…ë‹ˆë‹¤. GLYPH í•­ëª©ì´ ë°œê²¬ë˜ë©´, í•´ë‹¹ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ëŒ€ì²´í•˜ê¸° ìœ„í•´ì„œ OCRì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Python

```python
    def check_glyph_text(self, text: str, threshold: int = 1) -> bool:
        """í…ìŠ¤íŠ¸ì— GLYPH í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë©”ì„œë“œ"""
        if not text:
            return False

        # GLYPH í•­ëª©ì´ ìˆëŠ”ì§€ ì •ê·œì‹ìœ¼ë¡œ í™•ì¸
        matches = re.findall(r'GLYPH\w*', text)
        if len(matches) >= threshold:
            return True

        return False

    def check_glyphs(self, document: DoclingDocument) -> bool:
        """ë¬¸ì„œì— ê¸€ë¦¬í”„ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë©”ì„œë“œ"""
        for item, level in document.iterate_items():
            if isinstance(item, TextItem) and hasattr(item, 'prov') and item.prov:
                page_no = item.prov[0].page_no

                # GLYPH í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸. ì •ê·œì‹ì‚¬ìš©
                matches = re.findall(r'GLYPH\w*', item.text)
                if len(matches) > 10:
                    return True

        return False
```

**ì„¤ëª…:**

* `check_glyph_text(self, text: str, threshold: int = 1) -> bool`:
  * ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— "GLYPH" í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
  * ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ "GLYPH"ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ì°¾ì•„ë‚´ê³ , ê·¸ ê°œìˆ˜ê°€ `threshold` ì´ìƒì´ë©´ `True`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* `check_glyphs(self, document: DoclingDocument) -> bool`:
  * ì£¼ì–´ì§„ ë¬¸ì„œì— "GLYPH" í•­ëª©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
  * ë¬¸ì„œ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ í•­ëª©ì„ ê²€ì‚¬í•˜ì—¬ "GLYPH"ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ê°€ 10ê°œ ì´ìƒì´ë©´ `True`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

***

**`ocr_all_table_cells`**

í…Œì´ë¸”ì˜ ëª¨ë“  ì…€ì— ëŒ€í•´ GLYPH í•­ëª©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ OCRì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í…Œì´ë¸”ì—ì„œ ë¹„ì •ìƒì ìœ¼ë¡œ GLYPH í•­ëª©ì´ ë§ì´ ë°œìƒí•˜ëŠ” ê²½ìš°, ì„ë² ë”© í† í°ì´ í—ˆìš©ì¹˜ë¥¼ ë„˜ì–´ê°€ëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì¤‘ìš”í•œ ë¶€ë¶„ì…ë‹ˆë‹¤.

Python

```python
    def ocr_all_table_cells(self, document: DoclingDocument, pdf_path) -> List[Dict[str, Any]]:
        """
        ê¸€ë¦¬í”„ ê¹¨ì§„ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í…Œì´ë¸”ì— ëŒ€í•´ì„œë§Œ OCRì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        Args:
            document: DoclingDocument ê°ì²´
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        Returns:
            OCRì´ ì™„ë£Œëœ ë¬¸ì„œì˜ DoclingDocument ê°ì²´
        """
        try:
            import fitz
            import grpc
            import docling.models.ocr_pb2 as ocr_pb2
            import docling.models.ocr_pb2_grpc as ocr_pb2_grpc
            import itertools

            grpc_server_count = self.ocr_pipe_line_options.ocr_options.grpc_server_count

            PORTS = [50051 + i for i in range(grpc_server_count)]
            channels = [grpc.insecure_channel(f"localhost:{p}") for p in PORTS]
            stubs = [(ocr_pb2_grpc.OCRServiceStub(ch), p) for ch, p in zip(channels, PORTS)]
            rr = itertools.cycle(stubs)

            doc = fitz.open(pdf_path)

            for table_idx, table_item in enumerate(document.tables):
                if not table_item.data or not table_item.data.table_cells:
                    continue

                b_ocr = False
                for cell_idx, cell in enumerate(table_item.data.table_cells):
                    if self.check_glyph_text(cell.text, threshold=1):
                        b_ocr = True
                        break

                if b_ocr is False:
                    # ê¸€ë¦¬í”„ ê¹¨ì§„ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°, OCRì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
                    continue

                for cell_idx, cell in enumerate(table_item.data.table_cells):

                    # # Provenance ì •ë³´ì—ì„œ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                    if not table_item.prov:
                        continue

                    page_no = table_item.prov[0].page_no - 1
                    bbox = cell.bbox

                    page = doc.load_page(page_no)

                    # ì…€ì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í•´ë‹¹ ì˜ì—­ì„ ì˜ë¼ëƒ„
                    cell_bbox = fitz.Rect(
                        bbox.l, min(bbox.t, bbox.b),
                        bbox.r, max(bbox.t, bbox.b)
                    )

                    # bbox ë†’ì´ ê³„ì‚° (PDF ì¢Œí‘œê³„ ë‹¨ìœ„)
                    bbox_height = cell_bbox.height

                    # ëª©í‘œ í”½ì…€ ë†’ì´
                    target_height = 20

                    # zoom factor ê³„ì‚°
                    # (ë„ˆë¬´ ì‘ì€ bboxì¼ ê²½ìš° 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê±¸ ë°©ì§€)
                    zoom_factor = target_height / bbox_height if bbox_height > 0 else 1.0
                    zoom_factor = min(zoom_factor, 4.0)  # ìµœëŒ€ í™•ëŒ€ ë¹„ìœ¨ ì œí•œ
                    zoom_factor = max(zoom_factor, 1)  # ìµœì†Œ í™•ëŒ€ ë¹„ìœ¨ ì œí•œ

                    # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë Œë”ë§
                    mat = fitz.Matrix(zoom_factor, zoom_factor)
                    pix = page.get_pixmap(matrix=mat, clip=cell_bbox)
                    img_data = pix.tobytes("png")

                    # gRPC ì„œë²„ì™€ ì—°ê²°
                    # channel = grpc.insecure_channel('localhost:50051')
                    # stub = ocr_pb2_grpc.OCRServiceStub(channel)

                    # # OCR ìš”ì²­: ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ì „ì†¡
                    # response = stub.PerformOCR(ocr_pb2.OCRRequest(image_data=img_data))

                    req = ocr_pb2.OCRRequest(image_data=img_data)
                    stub, port = next(rr)  # ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ìŠ¤í… ì„ íƒ
                    response = stub.PerformOCR(req)

                    cell.text = ""
                    for result in response.results:
                        if len(cell.text) > 0:
                            cell.text += " "
                        cell.text += result.text if result else ""
        except grpc.RpcError as e:
            pass

        return document
```

**ì„¤ëª…:**

* `grpc_server_count`: ì‚¬ìš©í•  OCRìš© gRPC ì„œë²„ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. OCRìš© gRPC ì„œë²„ì˜ ìˆ˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ 4ê°œê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.
* `PORTS`: ê° gRPC ì„œë²„ì˜ í¬íŠ¸ ë²ˆí˜¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. 50051ë¶€í„° ì‹œì‘í•˜ì—¬ `grpc_server_count`ë§Œí¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
* `channels`: ê° gRPC ì„œë²„ì™€ì˜ ì—°ê²° ì±„ë„ì„ ìƒì„±í•©ë‹ˆë‹¤.
* `stubs`: ê° gRPC ì„œë²„ì— ëŒ€í•œ ìŠ¤í…ì„ ìƒì„±í•©ë‹ˆë‹¤.
* `rr`: ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ìŠ¤í…ì„ ì„ íƒí•˜ê¸° ìœ„í•œ ë°˜ë³µìì…ë‹ˆë‹¤.
* `for table_idx, table_item in enumerate(document.tables):`: ê° í˜ì´ì§€ì—ì„œ í…Œì´ë¸”ì„ ì°¾ì•„ í•´ë‹¹ í…Œì´ë¸”ì˜ ì…€ì„ ìˆœíšŒí•˜ë©° OCRì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  * `if not table_item.data or not table_item.data.table_cells:`: í…Œì´ë¸” ì•„ì´í…œì— ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ì…€ ëª©ë¡ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°, í•´ë‹¹ í…Œì´ë¸”ì€ ê±´ë„ˆëœë‹ˆë‹¤.
  * `for cell_idx, cell in enumerate(table_item.data.table_cells):`: ê° í…Œì´ë¸”ì˜ ì…€ì„ ìˆœíšŒí•©ë‹ˆë‹¤.
    * `if not table_item.prov:`: í…Œì´ë¸” ì•„ì´í…œì— ëŒ€í•œ Provenance ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, í•´ë‹¹ í…Œì´ë¸”ì€ ê±´ë„ˆëœë‹ˆë‹¤.
    * `if self.check_glyph_text(cell.text, threshold=1):`: ì…€ì˜ í…ìŠ¤íŠ¸ê°€ GLYPH í•­ëª©ì´ ì•„ë‹Œ ê²½ìš°, í•´ë‹¹ ì…€ì€ ê±´ë„ˆëœë‹ˆë‹¤.
    * ê° ì…€ì˜ ì˜ì—­ì„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§í•˜ê³ , gRPC ì„œë²„ì— OCR ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.

***

**`__call__`**

`DocumentProcessor` ì¸ìŠ¤í„´ìŠ¤ë¥¼ GenOS ì—ì„œ í˜¸ì¶œí• ë•Œì˜ ì§„ì…ì ìœ¼ë¡œ, í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí–ˆì„ ë•Œ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ ë¡œì§ì…ë‹ˆë‹¤. ë¬¸ì„œ ì²˜ë¦¬ì˜ ì „ì²´ íë¦„ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

Python

```python
    async def __call__(self, request: Request, file_path: str, **kwargs: dict):
        document: DoclingDocument = self.load_documents(file_path, **kwargs)

        if not check_document(document, self.enrichment_options) or self.check_glyphs(document):
            # OCRì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ OCR ìˆ˜í–‰
            document: DoclingDocument = self.load_documents_with_docling_ocr(file_path, **kwargs)

        # ê¸€ë¦¬í”„ ê¹¨ì§„ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í…Œì´ë¸”ì— ëŒ€í•´ì„œë§Œ OCR ìˆ˜í–‰ (ì²­í¬í† í° 8kì´ìƒ ë°œìƒ ë°©ì§€)
        document: DoclingDocument = self.ocr_all_table_cells(document, file_path)

        output_path, output_file = os.path.split(file_path)
        filename, _ = os.path.splitext(output_file)
        artifacts_dir = Path(f"{output_path}/{filename}")
        if artifacts_dir.is_absolute():
            reference_path = None
        else:
            reference_path = artifacts_dir.parent

        document = document._with_pictures_refs(image_dir=artifacts_dir, reference_path=reference_path)

        document = self.enrichment(document, **kwargs)

        has_text_items = False
        for item, _ in document.iterate_items():
            if (isinstance(item, (TextItem, ListItem, CodeItem, SectionHeaderItem)) and item.text and item.text.strip()) or (isinstance(item, TableItem) and item.data and len(item.data.table_cells) == 0):
                has_text_items = True
                break

        if has_text_items:
            # Extract Chunk from DoclingDocument
            chunks: List[DocChunk] = self.split_documents(document, **kwargs)
        else:
            # textê°€ ìˆëŠ” itemì´ ì—†ì„ ë•Œ documentì— ì„ì˜ì˜ text item ì¶”ê°€
            from docling_core.types.doc import ProvenanceItem

            # ì²« ë²ˆì§¸ í˜ì´ì§€ì˜ ê¸°ë³¸ ì •ë³´ ì‚¬ìš© (1-based indexing)
            page_no = 1

            # ProvenanceItem ìƒì„±
            prov = ProvenanceItem(
                page_no=page_no,
                bbox=BoundingBox(l=0, t=0, r=1, b=1),  # ìµœì†Œ bbox
                charspan=(0, 1)
            )

            # documentì— temp text item ì¶”ê°€
            document.add_text(
                label=DocItemLabel.TEXT,
                text=".",
                prov=prov
            )

            # split_documents í˜¸ì¶œ
            chunks: List[DocChunk] = self.split_documents(document, **kwargs)

        vectors = []
        if len(chunks) >= 1:
            vectors: list[dict] = self.compose_vectors(document, chunks, file_path, **kwargs)
        else:
            raise GenosServiceException(1, f"chunk length is 0")

        return vectors
```

**ì„¤ëª…:**

1. `document: DoclingDocument = self.load_documents(file_path, **kwargs)`: `load_documents` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì…ë ¥ëœ `file_path`ì˜ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. `**kwargs`ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥ UI ë¥¼ í†µí•´ì„œ ì§€ì •í•˜ê±°ë‚˜, í˜¹ì€ ìˆ˜ì§‘ê¸°ê°€ ìˆ˜ì§‘ë‹¨ì—ì„œ ì§€ì •í•œ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
2. `if not check_document(document, self.enrichment_options) or self.check_glyphs(document): ...`: ë¬¸ì„œê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ GLYPH í•­ëª©ì´ í¬í•¨ëœ ê²½ìš°, OCR ì²˜ë¦¬ë¥¼ ìœ„í•´ `load_documents_with_docling_ocr` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
3. `document: DoclingDocument = self.ocr_all_table_cells(document, file_path)`: ë¬¸ì„œ ë‚´ ëª¨ë“  í…Œì´ë¸” ì…€ì— ëŒ€í•´ OCR ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
4. ì•„í‹°íŒ©íŠ¸ ê²½ë¡œ ì„¤ì •:
   * `output_path, output_file = os.path.split(file_path)`: ì…ë ¥ íŒŒì¼ ê²½ë¡œì—ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œì™€ íŒŒì¼ ì´ë¦„ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
   * `filename, _ = os.path.splitext(output_file)`: íŒŒì¼ ì´ë¦„ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ ìˆœìˆ˜ íŒŒì¼ëª…ì„ ì–»ìŠµë‹ˆë‹¤.
   * `artifacts_dir = Path(f"{output_path}/{filename}")`: ë¬¸ì„œ ì²˜ë¦¬ ê³¼ì •ì—ì„œ ìƒì„±ë˜ëŠ” ì´ë¯¸ì§€ ë“±ì˜ ì•„í‹°íŒ©íŠ¸(ì¤‘ê°„ ê²°ê³¼ë¬¼)ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `input/mydoc.pdf` ë¼ë©´ `input/mydoc/` ì™€ ê°™ì€ ê²½ë¡œê°€ ë©ë‹ˆë‹¤.
   * `reference_path` ì„¤ì •: `artifacts_dir`ì´ ì ˆëŒ€ ê²½ë¡œì¸ì§€ ìƒëŒ€ ê²½ë¡œì¸ì§€ì— ë”°ë¼ ì´ë¯¸ì§€ ì°¸ì¡°ë¥¼ ìœ„í•œ ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
5. `document = document._with_pictures_refs(image_dir=artifacts_dir, reference_path=reference_path)`:
   * `DoclingDocument` ê°ì²´ ë‚´ì˜ ê·¸ë¦¼(PictureItem)ë“¤ì´ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥ë  ìœ„ì¹˜(`image_dir`)ì™€ ì°¸ì¡° ê²½ë¡œ(`reference_path`)ë¥¼ ì„¤ì •í•˜ì—¬, ê·¸ë¦¼ ê°ì²´ê°€ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ ì°¸ì¡°í•˜ë„ë¡ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. `PdfPipelineOptions`ì—ì„œ `generate_picture_images = True`ë¡œ ì„¤ì •ëœ ê²½ìš°, `docling` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì´ ê²½ë¡œì— ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•˜ê³ , ì´ ë©”ì„œë“œë¥¼ í†µí•´ ë¬¸ì„œ ê°ì²´ ë‚´ì˜ ì°¸ì¡°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
6. `document = self.enrichment(document, **kwargs)`: ë¬¸ì„œ ê°ì²´ì— ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤.
7. `for item, _ in document.items(): ...`: ë¬¸ì„œ ë‚´ì˜ ëª¨ë“  í•­ëª©ì— ëŒ€í•´ í•„ìˆ˜ í…ìŠ¤íŠ¸ í•­ëª©ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
   * ë§Œì•½ í…ìŠ¤íŠ¸ í•­ëª©ì´ í•˜ë‚˜ë¼ë„ ì¡´ì¬í•˜ë©´ `has_text_items`ë¥¼ `True`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
8. `chunks: List[DocChunk] = self.split_documents(document, **kwargs)`: ì—…ë°ì´íŠ¸ëœ `document` ê°ì²´ë¥¼ `split_documents` ë©”ì„œë“œì— ì „ë‹¬í•˜ì—¬ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    * textê°€ ìˆëŠ” itemì´ ì—†ì„ ë•Œ documentì— ì„ì˜ì˜ text item ì¶”ê°€í•©ë‹ˆë‹¤.
9. `vectors = [] ...`:
   * ë§Œì•½ ìƒì„±ëœ ì²­í¬ê°€ 1ê°œ ì´ìƒì´ë©´ (`len(chunks) >= 1`), `compose_vectors` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë©”íƒ€ë°ì´í„° ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
   * ì²­í¬ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ `GenosServiceException`ì„ ë°œìƒì‹œì¼œ ì˜¤ë¥˜ ìƒí™©ì„ì„ ì•Œë¦½ë‹ˆë‹¤.
10.  `return vectors`: ìƒì„±ëœ ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* `**kwargs` í™œìš©: `__call__` ë©”ì„œë“œì— ì „ë‹¬ë˜ëŠ” `**kwargs`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ `load_documents`, `split_documents`, `compose_vectors`ë¡œ ì „íŒŒë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¬¸ì„œ ì²˜ë¦¬ ì „ ê³¼ì •ì— ê±¸ì³ ë™ì ì¸ ì„¤ì •ì„ ì£¼ì…í•˜ëŠ” í†µë¡œë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, API ìš”ì²­ìœ¼ë¡œë¶€í„° íŠ¹ì • íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì•„ `kwargs`ë¡œ ì „ë‹¬í•˜ê³ , ì´ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ `PdfPipelineOptions`ì˜ ì¼ë¶€ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ `compose_vectors`ì—ì„œ íŠ¹ì • ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€/ì œì™¸í•˜ëŠ” ë“±ì˜ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ì•„í‹°íŒ©íŠ¸ ê´€ë¦¬: `artifacts_dir` ìƒì„± ê·œì¹™ì´ë‚˜ `reference_path` ì„¤ì • ë°©ì‹ì„ ë³€ê²½í•˜ì—¬, ìƒì„±ë˜ëŠ” ì¤‘ê°„ íŒŒì¼ë“¤ì˜ ì €ì¥ ìœ„ì¹˜ ë° ì°¸ì¡° ë°©ì‹ì„ ì¡°ì§ì˜ ì •ì±…ì— ë§ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### âœ¨ ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸

* **ë©”íƒ€ í•„ë“œ í™•ì¥**: `GenOSVectorMeta`ì™€ `Builder` í´ë˜ìŠ¤ì— í•„ë“œ ì¶”ê°€
* **í˜ì´ì§€ ì²˜ë¦¬ ë¡œì§ ìˆ˜ì •**: `set_page_info` íŒŒë¼ë¯¸í„° ì¡°ì •
* **ì²­í¬ ë¶„í•  ì»¤ìŠ¤í„°ë§ˆì´ì§•**:  `HybridChunker` íŒŒë¼ë¯¸í„° ê¸°ì¤€ê°’ ìˆ˜ì •
* **ë¬¸ìì—´ ë³€í™˜**: `NaN`, `None` ë“± ê°’ì€ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬

### âœ… ìœ ì§€ë³´ìˆ˜ íŒ

* Pydantic `extra='allow'` ì„¤ì •ìœ¼ë¡œ í•„ë“œ ë³€ê²½ì´ ìœ ì—°í•˜ê²Œ í—ˆìš©ë¨
* Builder íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ í•„ë“œ ì„¤ì • ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê³  ìœ ì§€ë³´ìˆ˜ë¥¼ ë‹¨ìˆœí™”

***

ì´ì™€ ê°™ì´ ì½”ë“œ ì¡°ê°ê³¼ í•¨ê»˜ ì„¤ëª…ì„ ë³´ë©´ì„œ `DocumentProcessor`ì™€ `GenOSVectorMetaBuilder`ì˜ ì‘ë™ ë°©ì‹ê³¼ ì‚¬ìš©ì ì •ì˜ ì§€ì ì„ íŒŒì•…í•˜ì‹œë©´, ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜ì •í•˜ê³  í™•ì¥í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
