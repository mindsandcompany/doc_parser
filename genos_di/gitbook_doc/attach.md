---
description: >-
  Genos ëŠ” í¬ê²Œ ì ì¬ìš©(ë‚´ë¶€), ì ì¬ìš©(ì™¸ë¶€), ì ì¬ìš©(ê·œì •), ì²¨ë¶€ìš© 4ê°€ì§€ ìœ í˜•ì˜ ì „ì²˜ë¦¬ê¸° (document parser)ë¥¼
  ì§€ì›í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì²¨ë¶€ìš© Intelligent Doc Parser - text ì¶”ì¶œí˜• ì „ì²˜ë¦¬ê¸°ì˜ ì½”ë“œ ì›í˜•ì— ëŒ€í•´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.
icon: forward
---

# ì²¨ë¶€ìš© ë¬¸ì„œ ì „ì²˜ë¦¬ê¸°

<figure><img src="../../../../.gitbook/assets/preprocess_code.png" alt=""><figcaption><p>ì „ì²˜ë¦¬ê¸° ìƒì„¸ì—ì„œ ì•„ë˜ ì½”ë“œë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></figcaption></figure>

***

ì—¬ê¸°ì„œëŠ” Genos ì²¨ë¶€ìš© ë¬¸ì„œ íŒŒì„œì˜ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë‚´ ì£¼ìš” êµ¬ì„± ìš”ì†Œì— ëŒ€í•œ ì½”ë“œ ì¤‘ì‹¬ì˜ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. ì½”ë“œ ì¡°ê°ê³¼ í•¨ê»˜ ê° ë¶€ë¶„ì˜ ê¸°ëŠ¥ì„ ì´í•´í•¨ìœ¼ë¡œì¨, íŠ¹ì • ìš”êµ¬ ì‚¬í•­ ë° ë¬¸ì„œ ìœ í˜•ì— ë§ê²Œ ë¬¸ì„œ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ”§ ê³µí†µ êµ¬ì„±ìš”ì†Œ

#### `GenOSVectorMetaBuilder` ë° `GenOSVectorMeta`

`GenOSVectorMetaBuilder`ëŠ” ê° ì²­í¬ì— ëŒ€í•œ ìƒì„¸ ë©”íƒ€ë°ì´í„° ê°ì²´ì¸ `GenOSVectorMeta`ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

**`GenOSVectorMeta` (Pydantic ëª¨ë¸)**

ë¨¼ì €, ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ë  ë©”íƒ€ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸ì…ë‹ˆë‹¤.

Python

```python
class GenOSVectorMeta(BaseModel):
    class Config:
        extra = 'allow' # Pydantic v2ì—ì„œëŠ” extra='allow' ëŒ€ì‹  model_config ì‚¬ìš© ê°€ëŠ¥

    text: str = None
    n_char: int = None
    n_word: int = None
    n_line: int = None
    e_page: int = None
    i_page: int = None
    i_chunk_on_page: int = None
    n_chunk_of_page: int = None
    i_chunk_on_doc: int = None
    n_chunk_of_doc: int = None
    n_page: int = None
    reg_date: str = None
    chunk_bboxes: str = None
    media_files: str = None
```

**ì„¤ëª…:**

* `BaseModel`ì„ ìƒì†ë°›ì•„ Pydantic ëª¨ë¸ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ì§ë ¬í™”/ì—­ì§ë ¬í™”ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.
* `Config.extra = 'allow'`: ëª¨ë¸ì— ì •ì˜ë˜ì§€ ì•Šì€ ì¶”ê°€ í•„ë“œê°€ ì…ë ¥ ë°ì´í„°ì— ì¡´ì¬í•˜ë”ë¼ë„ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  í—ˆìš©í•©ë‹ˆë‹¤. (Pydantic V2ì—ì„œëŠ” `model_config = ConfigDict(extra='allow')` í˜•íƒœë¡œ ì‚¬ìš©)
* ê° í•„ë“œëŠ” ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° í•­ëª©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
  * `text`: ì²­í¬ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©.
  * `n_char`, `n_word`, `n_line`: ë¬¸ì ìˆ˜, ë‹¨ì–´ ìˆ˜, ì¤„ ìˆ˜.
  * `e_page`, `i_page`, `i_chunk_on_page`, `n_chunk_of_page`: í˜ì´ì§€ ë‚´ì—ì„œì˜ ì²­í¬ ìœ„ì¹˜ ì •ë³´.
  * `i_chunk_on_doc`, `n_chunk_of_doc`: ë¬¸ì„œ ì „ì²´ì—ì„œì˜ ì²­í¬ ìœ„ì¹˜ ì •ë³´.
  * `n_page`: ë¬¸ì„œì˜ ì´ í˜ì´ì§€ ìˆ˜.
  * `reg_date`: ì²˜ë¦¬ ë“±ë¡ ì‹œê°„.
  * `bboxes`: í˜ì´ì§€ ë‚´ í•´ë‹¹ ì²­í¬ì˜ ê²½ê³„ ìƒì (JSON ë¬¸ìì—´ í˜•íƒœ).
  * `chunk_bboxes`: ì²­í¬ë¥¼ êµ¬ì„±í•˜ëŠ” ê° `DocItem`ì˜ ìƒì„¸ ê²½ê³„ ìƒì ì •ë³´ ë¦¬ìŠ¤íŠ¸.
  * `media_files`: ì²­í¬ ë‚´ í¬í•¨ëœ ë¯¸ë””ì–´ íŒŒì¼(ì´ë¯¸ì§€) ì •ë³´ ë¦¬ìŠ¤íŠ¸.

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* ê³ ê°ì‚¬ì—ì„œ í•„ìš”í•œ ì¶”ê°€ì ì¸ ë©”íƒ€ë°ì´í„° í•­ëª©ì´ ìˆë‹¤ë©´, ì´ `GenOSVectorMeta` ëª¨ë¸ì— ìƒˆë¡œìš´ í•„ë“œë¥¼ ì¶”ê°€ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* í•„ë“œ íƒ€ì…ì„ ë³´ë‹¤ ì—„ê²©í•˜ê²Œ ì •ì˜í•˜ê±°ë‚˜ (ì˜ˆ: `Optional[str]`), ê¸°ë³¸ê°’ì„ ì„¤ì •í•˜ê±°ë‚˜, ìœ íš¨ì„± ê²€ì‚¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

**`GenOSVectorMetaBuilder` í´ë˜ìŠ¤ ë° ì£¼ìš” ë©”ì„œë“œ**

`GenOSVectorMeta` ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ë¹Œë” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

Python

```python
class GenOSVectorMetaBuilder:
    def __init__(self):
        """ë¹Œë” ì´ˆê¸°í™”"""
        self.text: Optional[str] = None
        self.n_char: Optional[int] = None
        # ... (ë‹¤ë¥¸ í•„ë“œë“¤ë„ ì´ˆê¸°í™”) ...

    def set_text(self, text: str) -> "GenOSVectorMetaBuilder":
        """í…ìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ì„¤ì •"""
        self.text = text
        self.n_char = len(text)
        self.n_word = len(text.split())
        self.n_line = len(text.splitlines())
        return self

    def set_page_info(
            self, i_page: int, i_chunk_on_page: int, n_chunk_of_page: int
    ) -> "GenOSVectorMetaBuilder":
        """í˜ì´ì§€ ì •ë³´ ì„¤ì •"""
        self.i_page = i_page
        self.i_chunk_on_page = i_chunk_on_page
        self.n_chunk_of_page = n_chunk_of_page
        return self

    def set_chunk_index(self, i_chunk_on_doc: int) -> "GenOSVectorMetaBuilder":
        """ë¬¸ì„œ ì „ì²´ì˜ ì²­í¬ ì¸ë±ìŠ¤ ì„¤ì •"""
        self.i_chunk_on_doc = i_chunk_on_doc
        return self

    def set_global_metadata(self, **global_metadata) -> "GenOSVectorMetaBuilder":
        """ê¸€ë¡œë²Œ ë©”íƒ€ë°ì´í„° ë³‘í•©"""
        for key, value in global_metadata.items():
            if hasattr(self, key): # ë¹Œë” ë‚´ì— í•´ë‹¹ ì†ì„±ì´ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                setattr(self, key, value)
        return self

    def set_chunk_bboxes(self, doc_items: list, document: DoclingDocument) -> "GenOSVectorMetaBuilder":
        chunk_bboxes = []
        for item in doc_items:
            for prov in item.prov:
                label = item.self_ref
                type_ = item.label
                size = document.pages.get(prov.page_no).size
                page_no = prov.page_no
                bbox = prov.bbox
                bbox_data = {
                    'l': bbox.l / size.width,
                    't': bbox.t / size.height,
                    'r': bbox.r / size.width,
                    'b': bbox.b / size.height,
                    'coord_origin': bbox.coord_origin.value
                }
                chunk_bboxes.append({
                    'page': page_no,
                    'bbox': bbox_data,
                    'type': type_,
                    'ref': label
                })
        self.e_page = max([bbox['page'] for bbox in chunk_bboxes]) if chunk_bboxes else None
        self.chunk_bboxes = json.dumps(chunk_bboxes)
        return self

    def set_media_files(self, doc_items: list) -> "GenOSVectorMetaBuilder":
        temp_list = []
        if not doc_items:
            self.media_files = ""
            return self
        for item in doc_items:
            if isinstance(item, PictureItem):
                path = str(item.image.uri)
                name = path.rsplit("/", 1)[-1]
                temp_list.append({'name': name, 'type': 'image', 'ref': item.self_ref})
        self.media_files = json.dumps(temp_list)
        return self

    def build(self) -> GenOSVectorMeta:
        """ì„¤ì •ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ìµœì¢…ì ìœ¼ë¡œ GenOSVectorMeta ê°ì²´ ìƒì„±"""
        return GenOSVectorMeta(
            text=self.text,
            n_char=self.n_char,
            # ... (ëª¨ë“  í•„ë“œë¥¼ GenOSVectorMeta ìƒì„±ìì— ì „ë‹¬) ...
        )

```

**ì„¤ëª…:**

* **`__init__`**: ë¹Œë” ë‚´ë¶€ì˜ ëª¨ë“  ì†ì„±ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ ì†ì„±ë“¤ì€ `GenOSVectorMeta`ì˜ í•„ë“œë“¤ê³¼ ëŒ€ë¶€ë¶„ ì¼ì¹˜í•©ë‹ˆë‹¤.
* **`set_text`**: ì²­í¬ì˜ í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •í•˜ê³ , ë¬¸ì ìˆ˜, ë‹¨ì–´ ìˆ˜, ì¤„ ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë‚´ë¶€ ì†ì„±ì— ì €ì¥í•©ë‹ˆë‹¤.
* **`set_page_info`**: í˜ì´ì§€ ë²ˆí˜¸, í˜ì´ì§€ ë‚´ ì²­í¬ ì¸ë±ìŠ¤, í˜ì´ì§€ ë‚´ ì´ ì²­í¬ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
* **`set_chunk_index`**: ë¬¸ì„œ ì „ì²´ì—ì„œì˜ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
* **`set_global_metadata`**: `DocumentProcessor.compose_vectors`ì—ì„œ ì „ë‹¬ë°›ì€ `global_metadata` ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë“¤ì„ ë¹Œë”ì˜ í•´ë‹¹ ì†ì„±ì— í• ë‹¹í•©ë‹ˆë‹¤. ë¹Œë” ë‚´ì— `global_metadata`ì˜ í‚¤ì™€ ë™ì¼í•œ ì´ë¦„ì˜ ì†ì„±ì´ ìˆì–´ì•¼ ê°’ì´ í• ë‹¹ë©ë‹ˆë‹¤.
* **`set_chunk_bboxes`**: ì²­í¬ë¥¼ êµ¬ì„±í•˜ëŠ” ëª¨ë“  `DocItem`ë“¤ì˜ ìƒì„¸í•œ ê²½ê³„ ìƒì ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ê° í•­ëª©ì€ í˜ì´ì§€ ë²ˆí˜¸, ì •ê·œí™”ëœ ì¢Œí‘œ(0\~1 ê°’), `DocItem`ì˜ íƒ€ì… ë° ì°¸ì¡° IDë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì •ê·œí™”ëœ ì¢Œí‘œëŠ” í˜ì´ì§€ í¬ê¸°ì— ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ë¯€ë¡œ, ë‹¤ì–‘í•œ í¬ê¸°ì˜ í˜ì´ì§€ì—ì„œë„ ì¼ê´€ë˜ê²Œ ìœ„ì¹˜ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **`set_media_files`**: ì²­í¬ ë‚´ì— `PictureItem`(ì´ë¯¸ì§€)ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, í•´ë‹¹ ì´ë¯¸ì§€ì˜ íŒŒì¼ ì´ë¦„, íƒ€ì…("image"), ì°¸ì¡° IDë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
* **`build`**: ì§€ê¸ˆê¹Œì§€ `set_...` ë©”ì„œë“œë“¤ì„ í†µí•´ ë¹Œë” ë‚´ë¶€ì— ì¶•ì ëœ ëª¨ë“  ì†ì„±ê°’ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ `GenOSVectorMeta` Pydantic ëª¨ë¸ ê°ì²´ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* `GenOSVectorMeta` ëª¨ë¸ì— ìƒˆë¡œìš´ í•„ë“œë¥¼ ì¶”ê°€í–ˆë‹¤ë©´, ì´ ë¹Œë”ì—ë„ í•´ë‹¹ í•„ë“œë¥¼ ìœ„í•œ ë‚´ë¶€ ì†ì„±ê³¼ `set_...` ë©”ì„œë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
* `build` ë©”ì„œë“œì—ì„œ `GenOSVectorMeta` ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë„ ì¸ìë¡œ ì „ë‹¬í•˜ë„ë¡ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
* íŠ¹ì • í•„ë“œê°’ì„ ì„¤ì •í•˜ê¸° ì „ì— ì¶”ê°€ì ì¸ ê°€ê³µ ë¡œì§(ì˜ˆ: ë‚ ì§œ í˜•ì‹ ë³€í™˜, íŠ¹ì • ì½”ë“œê°’ ë§¤í•‘ ë“±)ì´ í•„ìš”í•˜ë‹¤ë©´ í•´ë‹¹ `set_...` ë©”ì„œë“œ ë‚´ë¶€ì— êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

***

### ê³µí†µ ë¬¸ì„œ ë¡œë”

#### `HwpLoader`

hwp íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. HWP íŒŒì¼ì„ XHTMLë¡œ ë³€í™˜í•œ í›„ PDFë¡œ ì €ì¥í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ PyMuPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤.

Python

```python
class HwpLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.output_dir = os.path.join('/tmp', str(uuid.uuid4()))
        os.makedirs(self.output_dir, exist_ok=True)

    def load(self):
        try:
            subprocess.run(['hwp5html', self.file_path, '--output', self.output_dir], check=True, timeout=600)
            converted_file_path = os.path.join(self.output_dir, 'index.xhtml')
            pdf_save_path = self.file_path.replace('.hwp', '.pdf')
            HTML(converted_file_path).write_pdf(pdf_save_path)
            loader = PyMuPDFLoader(pdf_save_path)
            return loader.load()
        except Exception as e:
            print(f"Failed to convert {self.file_path} to XHTML")
            raise e
        finally:
            if os.path.exists(self.output_dir):
                shutil.rmtree(self.output_dir)
```

**ì„¤ëª…:**

* `HwpLoader`: HWP íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤. HWP íŒŒì¼ì„ XHTMLë¡œ ë³€í™˜í•œ í›„ PDFë¡œ ì €ì¥í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ PyMuPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤.

***

#### `TextLoader`

í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¸ì½”ë”©ì„ ì§€ì›í•˜ë©°, PDFë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

Python

```python
class TextLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.output_dir = os.path.join('/tmp', str(uuid.uuid4()))
        os.makedirs(self.output_dir, exist_ok=True)

    def load(self):
        try:
            # 1) ìƒ˜í”Œë¡œ ì¸ì½”ë”© ì¶”ì •(150ë°”ì´íŠ¸)
            with open(self.file_path, 'rb') as f:
                sample = f.read(150)
            enc = chardet.detect(sample).get('encoding') or ''
            encodings = [enc] if enc and enc.lower() not in ('ascii','unknown') else []
            encodings += ['utf-8', 'cp949', 'euc-kr', 'iso-8859-1', 'latin-1']
            # 2) ì „ì²´ íŒŒì¼ ë°”ì´íŠ¸/í…ìŠ¤íŠ¸ í™•ë³´
            with open(self.file_path, 'rb') as f:
                raw = f.read()

            content = None
            for e in encodings:
                try:
                    content = raw.decode(e)  # ì „ì²´ íŒŒì¼ë¡œ ë””ì½”ë”©
                    break
                except UnicodeDecodeError:
                    continue
            if content is None:
                content = raw.decode('utf-8', errors='replace')

            # 4) PDF ë³€í™˜ ìœ ì§€
            html = f"<html><meta charset='utf-8'><body><pre>{content}</pre></body></html>"
            html_path = os.path.join(self.output_dir, 'temp.html')
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html)
            pdf_path = (self.file_path
                        .replace('.txt', '.pdf')
                        .replace('.json', '.pdf'))
            if HTML:
                HTML(html_path).write_pdf(pdf_path)
                loader = PyMuPDFLoader(pdf_path)
                return loader.load()
            # PDFê°€ ë¶ˆê°€í•˜ë©´ Document ì§ì ‘ ë°˜í™˜ (ì›í˜• ìŠ¤í‚¤ë§ˆ ìœ ì§€)
            return [Document(page_content=content, metadata={'source': self.file_path, 'page': 0})]

        except Exception:
            # ì‹¤íŒ¨ ì‹œì—ë„ ìŠ¤í‚¤ë§ˆëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•´ ë°˜í™˜
            for e in ['utf-8', 'cp949', 'euc-kr', 'iso-8859-1']:
                try:
                    with open(self.file_path, 'r', encoding=e) as f:
                        content = f.read()
                    return [Document(page_content=content, metadata={'source': self.file_path, 'page': 0})]
                except UnicodeDecodeError:
                    continue
            with open(self.file_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            return [Document(page_content=content, metadata={'source': self.file_path, 'page': 0})]
        finally:
            if os.path.exists(self.output_dir):
                shutil.rmtree(self.output_dir)
```

**ì„¤ëª…:**

* `TextLoader`: í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¸ì½”ë”©ì„ ì§€ì›í•˜ë©°, PDFë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

***

#### `TabularLoader`

í‘œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. CSV, Excel ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ í‘œ ë°ì´í„°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

Python

```python
class TabularLoader:
    def __init__(self, file_path: str, ext: str):
        packages = ['openpyxl', 'chardet']
        install_packages(packages)

        self.file_path = file_path
        if ext == ".csv":
            self.data_dict = self.load_csv_documents(file_path)
        elif ext == ".xlsx":
            self.data_dict = self.load_xlsx_documents(file_path)
        else:
            print(f"[!] Inadequate extension for TabularLoader: {ext}")
            return

    def check_sql_dtypes(self, df):
        df = df.convert_dtypes()
        res = []
        for col in df.columns:
            # col_name = col.strip().replace(' ', '_')
            dtype = str(df.dtypes[col]).lower()

            if 'int' in dtype:
                if '64' in dtype:
                    sql_dtype = 'BIGINT'
                else:
                    sql_dtype = 'INT'
            elif 'float' in dtype:
                sql_dtype = 'FLOAT'
            elif 'bool' in dtype:
                sql_dtype = 'BOOLEAN'
            elif 'date' in dtype:
                sql_dtype = 'DATE'
                df[col] = df[col].astype(str)
            elif 'datetime' in dtype:
                sql_dtype = 'DATETIME'
                df[col] = df[col].astype(str)
            # else:
            #     max_len = df[col].str.len().max().item() + 10
            #     sql_dtype = f'VARCHAR({max_len})'
            else:
                lens = df[col].astype(str).str.len()
                max_len_val = lens.max()
                max_len = int(0 if pd.isna(max_len_val) else max_len_val) + 10
                sql_dtype = f'VARCHAR({max_len})'

            res.append([col, sql_dtype])

        return df, res

    def process_data_rows(self, data: dict):
        """Arg: data (keys: 'sheet_name', 'page_column', 'page_column_type', 'documents')"""

        rows = []
        for doc in data["documents"]:
            row = {}
            if 'int' in data["page_column_type"]:
                row[data["page_column"]] = int(doc.page_content)
            elif 'float' in data["page_column_type"]:
                row[data["page_column"]] = float(doc.page_content)
            elif 'bool' in data["page_column_type"]:
                if doc.page_content.lower() == 'true':
                    row[data["page_column"]] = True
                elif doc.page_content.lower() == 'false':
                    row[data["page_column"]] = False
                else:
                    raise ValueError(f"Invalid boolean string: {doc.page_content}")
            else:
                row[data["page_column"]] = doc.page_content

            row.update(doc.metadata)
            rows.append(row)

        processed_data = {"sheet_name": data["sheet_name"], "data_rows": rows, "data_types": data["dtypes"]}
        return processed_data

    def load_csv_documents(self, file_path: str, **kwargs: dict):
        import chardet

        with open(file_path, "rb") as f:
            raw_file = f.read(10000)
        enc_type = chardet.detect(raw_file)['encoding']
        df = pd.read_csv(file_path, encoding=enc_type, index_col=False)
        df = df.fillna('null') # csv íŒŒì¼ì—ì„œë„ xlsx íŒŒì¼ê³¼ ë™ì¼í•˜ê²Œ nullë¡œ ì±„ì›€
        df, dtypes_str = self.check_sql_dtypes(df)

        for i in range(len(df.columns)):
            try:
                col = df.columns[0]
                # col_type = str(type(col))
                col_type = str(df[col].dtype)
                df = df.astype({col: 'str'})
                break
            except:
                raise ValueError(
                    f"Any columns cannot be converted into the string type so that can't load LangChain Documents: {dtypes_str}")

        loader = DataFrameLoader(df, page_content_column=col)
        documents = loader.load()

        data = {
            "sheet_name": "table_1",
            "page_column": col,
            "page_column_type": col_type,
            "documents": documents,
            "dtypes": dtypes_str
        }
        data = self.process_data_rows(data)  # including only one sheet as it's a csv file
        data_dict = {"data": [data]}
        return data_dict

    def load_xlsx_documents(self, file_path: str, **kwargs: dict):
        dfs = pd.read_excel(file_path, sheet_name=None)
        sheets = []
        for sheet_name, df in dfs.items():
            df = df.fillna('null')
            df, dtypes_str = self.check_sql_dtypes(df)

            for i in range(len(df.columns)):
                try:
                    col = df.columns[0]
                    col_type = str(type(col))
                    df = df.astype({col: 'str'})
                    break
                except:
                    raise ValueError(
                        f"Any columns cannot be converted into string type so that can't load LangChain Documents: {dtypes_str}")

            loader = DataFrameLoader(df, page_content_column=col)
            documents = loader.load()

            sheet = {
                "sheet_name": sheet_name,
                "page_column": col,
                "page_column_type": col_type,
                "documents": documents,
                "dtypes": dtypes_str
            }
            sheets.append(sheet)

        data_dict = {"data": []}
        for sheet in sheets:
            data = self.process_data_rows(sheet)
            data_dict["data"].append(data)

        return data_dict

    def return_vectormeta_format(self):
        if not self.data_dict:
            return None

        text = "[DA] " + str(self.data_dict)  # Add a token to indicate this string is for data analysis
        vectors = [GenOSVectorMeta.model_validate({
            'text': text,
            'n_chars': 1,
            'n_words': 1,
            'n_lines': 1,
            'i_page': 1,
            'e_page': 1,
            'n_page': 1,
            'i_chunk_on_page': 1,
            'n_chunk_of_page': 1,
            'i_chunk_on_doc': 1,
            'reg_date': datetime.now().isoformat(timespec='seconds') + 'Z',
            'chunk_bboxes': ".",
            'media_files': "."
        })]
        return vectors
```

**ì„¤ëª…:**

* `check_sql_dtypes`: ë°ì´í„°í”„ë ˆì„ì˜ ê° ì—´ì— ëŒ€í•œ SQL ë°ì´í„° ìœ í˜•ì„ í™•ì¸í•˜ê³ , í•„ìš”í•œ ê²½ìš° í˜• ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* `process_data_rows`: ë°ì´í„° í–‰ì„ ì²˜ë¦¬í•˜ê³ , í•„ìš”í•œ ê²½ìš° ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* `load_csv_documents`: CSV ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í•„ìš”í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* `load_xlsx_documents`: Excel ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í•„ìš”í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* `return_vectormeta_format`: í˜„ì¬ ë°ì´í„°ì˜ ë©”íƒ€ ì •ë³´ë¥¼ ë²¡í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

***

#### `AudioLoader`

`AudioLoader` í´ë˜ìŠ¤ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ì²­í¬ë¡œ ë¶„í• í•˜ë©°, ê° ì²­í¬ì— ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ ì „ì‚¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

Python

```python
class AudioLoader:
    def __init__(self,
                 file_path: str,
                 req_url: str,
                 req_data: dict,
                 chunk_sec: int = 29,
                 tmp_path: str = '.',
                 ):
        self.file_path = file_path
        self.tmp_path = tmp_path
        self.chunk_sec = chunk_sec
        self.req_url = req_url
        self.req_data = req_data

    def split_file_as_chunks(self) -> list:
        audio = pydub.AudioSegment.from_file(self.file_path)
        chunk_len = self.chunk_sec * 1000
        n_chunks = math.ceil(len(audio) / chunk_len)

        for i in range(n_chunks):
            start_ms = i * chunk_len
            overlap_start_ms = start_ms - 300 if start_ms > 0 else start_ms
            end_ms = start_ms + chunk_len
            audio_chunk = audio[overlap_start_ms:end_ms]
            audio_chunk.export(os.path.join(self.tmp_path, "tmp_{}.wav".format(str(i))), format="wav")
        tmp_files = glob(os.path.join(self.tmp_path, "*.wav"))
        return tmp_files

    def transcribe_audio(self, file_path_lst: list):
        transcribed_text_chunks = []

        def _send_request(filepath: str):
            """Send a request to 'whisper' model served"""
            files = {
                'file': (filepath, open(filepath, 'rb'), 'audio/mp3'),
            }

            response = requests.post(self.req_url, data=self.req_data, files=files)
            text = response.json().get('text', ', ')
            transcribed_text_chunks.append({
                'file_name': os.path.basename(filepath),
                'text': text
            })

        # Send parallel requests
        threads = [threading.Thread(target=_send_request, args=(f,)) for f in file_path_lst]
        for t in threads: t.start()
        for t in threads: t.join()

        # Merge transcribed text snippets in order
        transcribed_text_chunks.sort(key=lambda x: x['file_name'])
        transcribed_text = "[AUDIO]" + ' '.join([t['text'] for t in transcribed_text_chunks])
        return transcribed_text

    def return_vectormeta_format(self):
        audio_chunks = self.split_file_as_chunks()
        transcribed_text = self.transcribe_audio(audio_chunks)
        res = [GenOSVectorMeta.model_validate({
            'text': transcribed_text,
            'n_chars': 1,
            'n_words': 1,
            'n_lines': 1,
            'i_page': 1,
            'e_page': 1,
            'n_page': 1,
            'i_chunk_on_page': 1,
            'n_chunk_of_page': 1,
            'i_chunk_on_doc': 1,
            'reg_date': datetime.now().isoformat(timespec='seconds') + 'Z',
            'chunk_bboxes': ".",
            'media_files': "."
        })]
        return res
```

**ì„¤ëª…:**

* `AudioLoader`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ì²­í¬ë¡œ ë¶„í• í•˜ë©°, ê° ì²­í¬ì— ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ ì „ì‚¬í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
  * `split_file_as_chunks`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì§€ì •ëœ ê¸¸ì´ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
  * `transcribe_audio`: ë¶„í• ëœ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì „ì‚¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
  * `return_vectormeta_format`: ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ë©”íƒ€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

***

#### `HWPX`

`HWPX` í´ë˜ìŠ¤ëŠ” HWPX í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , í•„ìš”í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. `HierarchicalChunker`, `HybridChunker`, `HwpxProcessor` ë“±ì˜ êµ¬ì„± ìš”ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³ , ê° ì²­í¬ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

##### ``HierarchicalChunker` ë° `HybridChunker`

`HierarchicalChunker`ëŠ” ë¬¸ì„œë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” ì—­í• ì„ í•˜ë©°, `HybridChunker`ëŠ” í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì„¹ì…˜ë³„ ì²­í¬ë¥¼ ë¶„í• í•˜ê³  ë³‘í•©í•˜ëŠ” ê³ ê¸‰ ì²­ì»¤ì…ë‹ˆë‹¤.

Python

```python
class HierarchicalChunker(BaseChunker):
    merge_list_items: bool = True

    def chunk(self, dl_doc: DLDocument, **kwargs: Any) -> Iterator[BaseChunk]:
        # ëª¨ë“  ì•„ì´í…œê³¼ í—¤ë” ì •ë³´ ìˆ˜ì§‘
        heading_by_level: dict[LevelNumber, str] = {}
        list_items: list[TextItem] = []
        for item, level in dl_doc.iterate_items():
            captions = None
            if isinstance(item, DocItem):
                # first handle any merging needed
                if self.merge_list_items:
                    if isinstance(
                            item, ListItem
                    ) or (  # TODO remove when all captured as ListItem:
                            isinstance(item, TextItem)
                            and item.label == DocItemLabel.LIST_ITEM
                    ):
                        list_items.append(item)
                        continue
                # ... í—¤ë” ì²˜ë¦¬ ë¡œì§

                c = DocChunk(
                    text=text,
                    meta=DocMeta(
                        doc_items=[item],
                        headings=[heading_by_level[k] for k in sorted(heading_by_level)] or None,
                        captions=captions,
                        origin=dl_doc.origin,
                    ),
                )
                yield c

        if self.merge_list_items and list_items:  # need to yield
            yield DocChunk(
                text=self.delim.join([i.text for i in list_items]),
                meta=DocMeta(
                    doc_items=list_items,
                    headings=[heading_by_level[k] for k in sorted(heading_by_level)] or None,
                    origin=dl_doc.origin,
                ),
            )

class HybridChunker(BaseChunker):

    def chunk(self, dl_doc: DoclingDocument, **kwargs: Any) -> Iterator[BaseChunk]:
        r"""Chunk the provided document.
        Args:
            dl_doc (DLDocument): document to chunk
        Yields:
            Iterator[Chunk]: iterator over extracted chunks
        """
        res: Iterable[DocChunk]
        res = self._inner_chunker.chunk(dl_doc=dl_doc, **kwargs)  # type: ignore
        res = [x for c in res for x in self._split_by_doc_items(c)]
        res = [x for c in res for x in self._split_using_plain_text(c)]

        if self.merge_peers:
            res = self._merge_chunks_with_matching_metadata(res)
        return iter(res)
```

***

##### `HwpxProcessor`

`HwpxProcessor` í´ë˜ìŠ¤ëŠ” HWPX í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° í•„ìš”í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” ë¬¸ì„œì˜ ì²­í¬ë¥¼ ìƒì„±í•˜ê³ , ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ë©°, ìµœì¢…ì ìœ¼ë¡œ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Python

```python
class HwpxProcessor:
    def __init__(self):
        self.page_chunk_counts = defaultdict(int)
        self.pipeline_options = PipelineOptions()
        self.pipeline_options.save_images = False
        self.converter = DocumentConverter(
            format_options={
                InputFormat.XML_HWPX: HwpxFormatOption(
                    pipeline_options=self.pipeline_options
                )
            }
        )

    def get_paths(self, file_path: str):
        output_path, output_file = os.path.split(file_path)
        filename, _ = os.path.splitext(output_file)
        artifacts_dir = Path(f"{output_path}/{filename}")
        if artifacts_dir.is_absolute():
            reference_path = None
        else:
            reference_path = artifacts_dir.parent
        return artifacts_dir, reference_path

    def get_media_files(self, doc_items: list):
        temp_list = []
        for item in doc_items:
            if isinstance(item, PictureItem):
                path = str(item.image.uri)
                name = path.rsplit("/", 1)[-1]
                temp_list.append({'path': path, 'name': name})
        return temp_list

    def safe_join(self, iterable):
        if not isinstance(iterable, (list, tuple, set)):
            return ''
        return ''.join(map(str, iterable)) + '\n'

    def load_documents(self, file_path: str, **kwargs: dict) -> DoclingDocument:
        save_images = kwargs.get('save_images', False)

        if self.pipeline_options.save_images != save_images:
            self.pipeline_options.save_images = save_images
            # self._create_converters()

        conv_result: ConversionResult = self.converter.convert(file_path, raises_on_error=True)
        return conv_result.document

    def split_documents(self, documents: DoclingDocument, **kwargs: dict) -> List[DocChunk]:
        chunker = HybridChunker(max_tokens=int(1e30), merge_peers=True)
        chunks: List[DocChunk] = list(chunker.chunk(dl_doc=documents, **kwargs))
        for chunk in chunks:
            self.page_chunk_counts[chunk.meta.doc_items[0].prov[0].page_no] += 1
        return chunks

    async def compose_vectors(self, document: DoclingDocument, chunks: List[DocChunk], file_path: str, request: Request,
                              **kwargs: dict) -> list[dict]:
        global_metadata = dict(
            n_chunk_of_doc=len(chunks),
            n_page=document.num_pages(),
            reg_date=datetime.now().isoformat(timespec='seconds') + 'Z',
        )

        current_page = None
        chunk_index_on_page = 0
        vectors = []
        upload_tasks = []
        for chunk_idx, chunk in enumerate(chunks):
            chunk_page = chunk.meta.doc_items[0].prov[0].page_no
            content = self.safe_join(chunk.meta.headings) + chunk.text

            if chunk_page != current_page:
                current_page = chunk_page
                chunk_index_on_page = 0

            vector = (GenOSVectorMetaBuilder()
                      .set_text(content)
                      .set_page_info(chunk_page, chunk_index_on_page, self.page_chunk_counts[chunk_page])
                      .set_chunk_index(chunk_idx)
                      .set_global_metadata(**global_metadata)
                      .set_chunk_bboxes(chunk.meta.doc_items, document)
                      .set_media_files(chunk.meta.doc_items)
                      ).build()
            vectors.append(vector)

            chunk_index_on_page += 1
            file_list = self.get_media_files(chunk.meta.doc_items)
            upload_tasks.append(asyncio.create_task(
                upload_files(file_list, request=request)
            ))

        if upload_tasks:
            await asyncio.gather(*upload_tasks)
        return vectors

    async def __call__(self, request: Request, file_path: str, **kwargs: dict):
        document: DoclingDocument = self.load_documents(file_path, **kwargs)
        artifacts_dir, reference_path = self.get_paths(file_path)
        document = document._with_pictures_refs(image_dir=artifacts_dir, reference_path=reference_path)

        chunks: list[DocChunk] = self.split_documents(document, **kwargs)

        vectors = []
        if len(chunks) >= 1:
            vectors: list[dict] = await self.compose_vectors(document, chunks, file_path, request, **kwargs)
        else:
            raise GenosServiceException(1, f"chunk length is 0")
        return vectors
```

**ì„¤ëª…:**

* `get_paths`: ì£¼ì–´ì§„ íŒŒì¼ ê²½ë¡œì— ëŒ€í•œ ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬ì™€ ì°¸ì¡° ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* `get_media_files`: ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ë¯¸ë””ì–´ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
* `safe_join`: ì£¼ì–´ì§„ iterableì˜ ìš”ì†Œë¥¼ ì•ˆì „í•˜ê²Œ ì—°ê²°í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
* `load_documents`: ì£¼ì–´ì§„ íŒŒì¼ ê²½ë¡œì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
* `split_documents`: ë¡œë“œëœ ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ì‘ì€ ë‹¨ìœ„(ì²­í¬)ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
  * `chunker: HybridChunker = HybridChunker()`: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•´ `HybridChunker` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. `HybridChunker`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ê³„ì¸µì  êµ¬ì¡°(Hierarchical)ì™€ ì˜ë¯¸ë¡ ì  ë¶„í• (Semantic, ì£¼ì„ ì²˜ë¦¬ëœ `semchunk` ì˜ì¡´ì„± ë¶€ë¶„ì—ì„œ ìœ ì¶”)ì„ ê²°í•©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤.&#x20;
    * `max_tokens`ëŠ” ê° ì²­í¬ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.
    * `merge_peers`ëŠ” ì¸ì ‘í•œ ì²­í¬ë“¤ì„ ë³‘í•©í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
  * `chunks: List[DocChunk] = list(chunker.chunk(dl_doc=documents, **kwargs))`: `chunker`ì˜ `chunk` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ `DoclingDocument`ë¥¼ `DocChunk` ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. `**kwargs`ëŠ” ì²­í‚¹ ê³¼ì •ì— í•„ìš”í•œ ì¶”ê°€ ì˜µì…˜ì„ ì „ë‹¬í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ .
  * `for chunk in chunks: self.page_chunk_counts[chunk.meta.doc_items[0].prov[0].page_no] += 1`: ê° ì²­í¬ê°€ ì–´ë–¤ í˜ì´ì§€ì—ì„œ ì™”ëŠ”ì§€ íŒŒì•…í•˜ì—¬ `self.page_chunk_counts` ë”•ì…”ë„ˆë¦¬ì— í˜ì´ì§€ë³„ ì²­í¬ ìˆ˜ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. ì´ëŠ” ì¶”í›„ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œ í™œìš©ë©ë‹ˆë‹¤. (`chunk.meta.doc_items[0].prov[0].page_no`ëŠ” ì²­í¬ë¥¼ êµ¬ì„±í•˜ëŠ” ì²«ë²ˆì§¸ ë¬¸ì„œ ì•„ì´í…œì˜ ì²«ë²ˆì§¸ ì¶œì²˜ ì •ë³´ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.)
* `compose_vectors`: ë¶„í• ëœ ì²­í¬ë“¤ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ìµœì¢…ì ì¸ ë²¡í„°(ë”•ì…”ë„ˆë¦¬ í˜•íƒœ) ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì´ ê³ ê°ì‚¬ë³„ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ëŠ” ë¶€ë¶„ì´ë©° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.
* `__call__`: ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
  * `document: DoclingDocument = self.load_documents(file_path, **kwargs)`: `load_documents` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì…ë ¥ëœ `file_path`ì˜ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. `**kwargs`ëŠ” ì‚¬ìš©ìê°€ ì…ë ¥ UI ë¥¼ í†µí•´ì„œ ì§€ì •í•˜ê±°ë‚˜, í˜¹ì€ ìˆ˜ì§‘ê¸°ê°€ ìˆ˜ì§‘ë‹¨ì—ì„œ ì§€ì •í•œ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
  * `artifacts_dir, reference_path = self.get_paths(file_path)`: ë¬¸ì„œì˜ ì•„í‹°íŒ©íŠ¸ ë””ë ‰í† ë¦¬ì™€ ì°¸ì¡° ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
  * `document = document._with_pictures_refs(image_dir=artifacts_dir, reference_path=reference_path)`: `DoclingDocument` ê°ì²´ ë‚´ì˜ ê·¸ë¦¼(PictureItem)ë“¤ì´ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥ë  ìœ„ì¹˜(`image_dir`)ì™€ ì°¸ì¡° ê²½ë¡œ(`reference_path`)ë¥¼ ì„¤ì •í•˜ì—¬, ê·¸ë¦¼ ê°ì²´ê°€ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ ì°¸ì¡°í•˜ë„ë¡ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. `PdfPipelineOptions`ì—ì„œ `generate_picture_images = True`ë¡œ ì„¤ì •ëœ ê²½ìš°, `docling` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì´ ê²½ë¡œì— ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•˜ê³ , ì´ ë©”ì„œë“œë¥¼ í†µí•´ ë¬¸ì„œ ê°ì²´ ë‚´ì˜ ì°¸ì¡°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
  * `chunks: List[DocChunk] = self.split_documents(document, **kwargs)`: ì—…ë°ì´íŠ¸ëœ `document` ê°ì²´ë¥¼ `split_documents` ë©”ì„œë“œì— ì „ë‹¬í•˜ì—¬ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    * textê°€ ìˆëŠ” itemì´ ì—†ì„ ë•Œ documentì— ì„ì˜ì˜ text item ì¶”ê°€í•©ë‹ˆë‹¤.
  * `vectors = [] ...`:
   * ë§Œì•½ ìƒì„±ëœ ì²­í¬ê°€ 1ê°œ ì´ìƒì´ë©´ (`len(chunks) >= 1`), `compose_vectors` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë©”íƒ€ë°ì´í„° ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
   * ì²­í¬ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ `GenosServiceException`ì„ ë°œìƒì‹œì¼œ ì˜¤ë¥˜ ìƒí™©ì„ì„ ì•Œë¦½ë‹ˆë‹¤.
  * `return vectors`: ìƒì„±ëœ ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

***

### ğŸ“‚ ê³µí†µ ì „ì²˜ë¦¬ íë¦„

#### `DocumentProcessor`

`DocumentProcessor` í´ë˜ìŠ¤ëŠ” Genos ì˜ ì „ì²˜ë¦¬ê¸°ê°€ í˜¸ì¶œë˜ëŠ” ê´€ë¬¸ì…ë‹ˆë‹¤. ë‚´ë¶€ êµ¬ì„±ì„ ë³´ë©´, ë¬¸ì„œë¥¼ ë¡œë“œ, ë³€í™˜, ë¶„í• í•˜ê³  ê° ë¶€ë¶„ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ëŠ” í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

**`__init__` (ì´ˆê¸°í™”)**

`DocumentProcessor` ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë  ë•Œ í˜¸ì¶œë˜ëŠ” ì´ˆê¸°í™” ë©”ì„œë“œì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì£¼ìš” ì„¤ì •ë“¤ì´ ì •ì˜ë©ë‹ˆë‹¤.

Python

```python
class DocumentProcessor:

    def __init__(self):
        self.page_chunk_counts = defaultdict(int)
        self.hwpx_processor = HwpxProcessor()
```

**ì„¤ëª…:**

* `self.page_chunk_counts = defaultdict(int)`: í˜ì´ì§€ë³„ë¡œ ìƒì„±ëœ ì²­í¬ì˜ ìˆ˜ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
* `self.hwpx_processor = HwpxProcessor()`: HWPX ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ì„œì…ë‹ˆë‹¤.

***

**`get_loader`**

ë¬¸ì„œ ë³€í™˜ì— ì‚¬ìš©í•  ë³€í™˜ê¸°ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

Python

```python
    def get_loader(self, file_path: str):
        ext = os.path.splitext(file_path)[-1].lower()
        real_type = self.get_real_file_type(file_path)

        # í™•ì¥ìì™€ ì‹¤ì œ íŒŒì¼ íƒ€ì…ì´ ë‹¤ë¥¼ ë•Œë§Œ real_type ì‚¬ìš©
        if ext != real_type and real_type == 'pdf':
            return PyMuPDFLoader(file_path)
        elif ext != real_type and real_type in ['txt', 'json', 'md']:
            return TextLoader(file_path)
        # ì›ë˜ í™•ì¥ì ê¸°ë°˜ ë¡œì§
        elif ext == '.pdf':
            return PyMuPDFLoader(file_path)
        elif ext in ['.doc', '.docx']:
            return UnstructuredWordDocumentLoader(file_path)
        elif ext in ['.ppt', '.pptx']:
            return UnstructuredPowerPointLoader(file_path)
        elif ext in ['.jpg', '.jpeg', '.png']:
            return UnstructuredImageLoader(file_path)
        elif ext in ['.txt', '.json', '.md']:
            return TextLoader(file_path)
        elif ext == '.hwp':
            return HwpLoader(file_path)
        elif ext == '.md':
            return UnstructuredMarkdownLoader(file_path)
        else:
            return UnstructuredFileLoader(file_path)
```

**ì„¤ëª…:**

* ê° í™•ì¥ìì— ë§ëŠ” ë¡œë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

***

**`convert_to_pdf`, `convert_md_to_pdf`**

ë¬¸ì„œë¥¼ PDFë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.

Python

```python
    def convert_to_pdf(self, file_path: str):
        out_path = "."
        try:
            subprocess.run(['soffice', '--headless', '--convert-to', 'pdf', '--outdir', out_path, file_path],
                           check=True)
            pdf_path = os.path.basename(file_path).replace(file_path.split('.')[-1], 'pdf')
            return pdf_path
        except subprocess.CalledProcessError as e:
            print(f"Error converting PPT to PDF: {e}")
            return False

    def convert_md_to_pdf(self, md_path):
        """Markdown íŒŒì¼ì„ PDFë¡œ ë³€í™˜"""
        install_packages(['chardet'])
        import chardet

        pdf_path = md_path.replace('.md', '.pdf')
        with open(md_path, 'rb') as f:
            raw_file = f.read(100)
        enc_type = chardet.detect(raw_file)['encoding']
        with open(md_path, 'r', encoding=enc_type) as f:
            md_content = f.read()

        html_content = markdown(md_content)
        HTML(string=html_content).write_pdf(pdf_path)
        return pdf_path
```

**ì„¤ëª…:**

* `convert_to_pdf`:
  * ì£¼ì–´ì§„ íŒŒì¼ ê²½ë¡œì— ìˆëŠ” ë¬¸ì„œë¥¼ PDFë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
  * LibreOfficeì˜ ëª…ë ¹ì¤„ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* `convert_md_to_pdf`:
  * Markdown íŒŒì¼ì„ PDFë¡œ ë³€í™˜í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.

**`load_documents`**

ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.

Python

```python
    def load_documents(self, file_path: str, **kwargs: dict) -> list[Document]:
        loader = self.get_loader(file_path)
        documents = loader.load()
        return documents
```

**ì„¤ëª…:**

* `loader = self.get_loader(file_path)`: íŒŒì¼ ê²½ë¡œì— ë§ëŠ” ë¡œë”ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
* `documents = loader.load()`: ë¡œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
* `return documents`: ë¡œë“œëœ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

***

**`split_documents`**

ë¡œë“œëœ ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ì‘ì€ ë‹¨ìœ„(ì²­í¬)ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

Python

```python
    def split_documents(self, documents, **kwargs: dict) -> list[Document]:
        text_splitter = RecursiveCharacterTextSplitter(**kwargs)
        chunks = text_splitter.split_documents(documents)
        chunks = [chunk for chunk in chunks if chunk.page_content]
        if not chunks:
            raise Exception('Empty document')

        for chunk in chunks:
            page = chunk.metadata.get('page', 0)
            self.page_chunk_counts[page] += 1
        return chunks
```

**ì„¤ëª…:**

* `text_splitter = RecursiveCharacterTextSplitter(**kwargs)`: ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê¸° ìœ„í•´ `RecursiveCharacterTextSplitter` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” ì¬ê·€ì ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë¶„í• í•˜ì—¬ ê° ì²­í¬ì˜ ë‚´ìš©ì„ ìµœëŒ€í•œ ìœ ì§€í•©ë‹ˆë‹¤.
* `chunks = text_splitter.split_documents(documents)`: `text_splitter`ì˜ `split_documents` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
* `chunks = [chunk for chunk in chunks if chunk.page_content]`: í˜ì´ì§€ ì½˜í…ì¸ ê°€ ìˆëŠ” ì²­í¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
* `if not chunks: raise Exception('Empty document')`: ì²­í¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
* `for chunk in chunks: page = chunk.metadata.get('page', 0); self.page_chunk_counts[page] += 1`: ê° ì²­í¬ì˜ í˜ì´ì§€ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜ì´ì§€ë³„ ì²­í¬ ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
* `return chunks`: ìµœì¢… ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

***

**`compose_vectors`**

ë¶„í• ëœ ì²­í¬ë“¤ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ìµœì¢…ì ì¸ ë²¡í„°(ë”•ì…”ë„ˆë¦¬ í˜•íƒœ) ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì´ ê³ ê°ì‚¬ë³„ ìš”êµ¬ì‚¬í•­ì„ ë°˜ì˜í•˜ëŠ” ë¶€ë¶„ì´ë©° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

Python

```python
    def compose_vectors(self, file_path: str, chunks: list[Document], **kwargs: dict) -> list[dict]:
        ext = os.path.splitext(file_path)[-1].lower()
        real_type = self.get_real_file_type(file_path)

        # í™•ì¥ìì™€ ì‹¤ì œ íŒŒì¼ íƒ€ì…ì´ ë‹¤ë¥¼ ë•Œë§Œ real_type ì‚¬ìš©
        if ext != real_type and real_type == 'pdf':
            pdf_path = file_path
        elif ext != real_type and real_type in ['txt', 'json', 'md']:
            # pdf_path = None  # PDF ë³€í™˜ ì—†ì´ ì§ì ‘ ì²˜ë¦¬
            pdf_path = file_path.replace('.hwp', '.pdf').replace('.txt', '.pdf').replace('.json', '.pdf')
        # ì›ë˜ í™•ì¥ì ê¸°ë°˜ ë¡œì§
        elif file_path.endswith('.md'):
            pdf_path = self.convert_md_to_pdf(file_path)
        elif file_path.endswith('.ppt'):
            pdf_path = self.convert_to_pdf(file_path)
            if not pdf_path:
                return False
        else:
            pdf_path = file_path.replace('.hwp', '.pdf').replace('.txt', '.pdf').replace('.json', '.pdf')

        doc = fitz.open(pdf_path) if (pdf_path and os.path.exists(pdf_path)) else None

        if file_path.endswith('.ppt'):
            if os.path.exists(pdf_path):
                subprocess.run(["rm", pdf_path], check=True)

        global_metadata = dict(
            n_chunk_of_doc=len(chunks),
            n_page=max([chunk.metadata.get('page', 0) for chunk in chunks]),
            reg_date=datetime.now().isoformat(timespec='seconds') + 'Z'
        )
        current_page = None
        chunk_index_on_page = 0

        vectors = []
        for chunk_idx, chunk in enumerate(chunks):
            page = chunk.metadata.get('page', 0)
            text = chunk.page_content

            if page != current_page:
                current_page = page
                chunk_index_on_page = 0

            if doc:
                fitz_page = doc.load_page(page)
                global_metadata['chunk_bboxes'] = json.dumps(merge_overlapping_bboxes([{
                    'page': page + 1,
                    'type': 'text',
                    'bbox': {
                        'l': rect[0] / fitz_page.rect.width,
                        't': rect[1] / fitz_page.rect.height,
                        'r': rect[2] / fitz_page.rect.width,
                        'b': rect[3] / fitz_page.rect.height,
                    }
                } for rect in fitz_page.search_for(text)], x_tolerance=1 / fitz_page.rect.width,
                    y_tolerance=1 / fitz_page.rect.height))

            vectors.append(GenOSVectorMeta.model_validate({
                'text': text,
                'n_char': len(text),
                'n_word': len(text.split()),
                'n_line': len(text.splitlines()),
                'i_page': page,
                'e_page': page,
                'i_chunk_on_page': chunk_index_on_page,
                'n_chunk_of_page': self.page_chunk_counts[page],
                'i_chunk_on_doc': chunk_idx,
                **global_metadata
            }))
            chunk_index_on_page += 1

        return vectors
```

**ì„¤ëª…:**

**í•œêµ­ì€í–‰ ê¸°ë¡ë¬¼ì˜ ë©”íƒ€ë°ì´íƒ€ Mapping ì˜ˆ**

* **`global_metadata`**: ë¬¸ì„œ ì „ì²´ì— ê³µí†µì ìœ¼ë¡œ ì ìš©ë  ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
  * `n_chunk_of_doc=len(chunks)`: ë¬¸ì„œ ë‚´ ì´ ì²­í¬ ìˆ˜.
  * `n_page=max([chunk.metadata.get('page', 0) for chunk in chunks])`: ë¬¸ì„œì˜ ì´ í˜ì´ì§€ ìˆ˜.
  * `reg_date`: í˜„ì¬ ì‹œê°„ì„ ISO í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë“±ë¡ì¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
* ë£¨í”„ (`for chunk_idx, chunk in enumerate(chunks):`): ê° ì²­í¬ë¥¼ ìˆœíšŒí•˜ë©° ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  * `page = chunk.metadata.get('page', 0)`: í˜„ì¬ ì²­í¬ì˜ ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
  * `text = chunk.page_content`: ì²­í¬ì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
  * `global_metadata['chunk_bboxes'] = json.dumps(merge_overlapping_bboxes(...)`: ì²­í¬ì˜ ê²½ê³„ ìƒì ì •ë³´ë¥¼ ë³‘í•©í•˜ì—¬ JSON ë¬¸ìì—´ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
  * `vectors.append(...)`: ìƒì„±ëœ `GenOSVectorMeta` Pydantic ëª¨ë¸ ê°ì²´ë¥¼ `vectors` ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
* í˜ì´ì§€ ë³€ê²½ ê°ì§€ ë¡œì§: `current_page`ì™€ `chunk_index_on_page`ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ê°€ ë°”ë€” ë•Œë§ˆë‹¤ í˜ì´ì§€ ë‚´ ì²­í¬ ì¸ë±ìŠ¤ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

***

**`__call__`**

`DocumentProcessor` ì¸ìŠ¤í„´ìŠ¤ë¥¼ GenOS ì—ì„œ í˜¸ì¶œí• ë•Œì˜ ì§„ì…ì ìœ¼ë¡œ, í•¨ìˆ˜ì²˜ëŸ¼ í˜¸ì¶œí–ˆì„ ë•Œ ì‹¤í–‰ë˜ëŠ” ë©”ì¸ ë¡œì§ì…ë‹ˆë‹¤. ë¬¸ì„œ ì²˜ë¦¬ì˜ ì „ì²´ íë¦„ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

Python

```python
    async def __call__(self, request: Request, file_path: str, **kwargs: dict):
        ext = os.path.splitext(file_path)[-1].lower()
        if ext in ('.wav', '.mp3', '.m4a'):
            # Generate a temporal path saving audio chunks: the audio file is supposed to be splited to several chunks due to limitted length by the model
            tmp_path = "./tmp_audios_{}".format(os.path.basename(file_path).split('.')[0])
            if not os.path.exists(tmp_path):
                os.makedirs(tmp_path)

            # Use 'Whisper' model served in-house
            # [!] Modify the request parameters to change a STT model to be used
            loader = AudioLoader(
                file_path=file_path,
                req_url="http://192.168.74.164:30100/v1/audio/transcriptions",
                req_data={
                    'model': 'model',
                    'language': 'ko',
                    'response_format': 'json',
                    'temperature': '0',
                    'stream': 'false',
                    'timestamp_granularities[]': 'word'
                },
                chunk_sec=29,  # length(sec) of a chunk from the uploaded audio
                tmp_path=tmp_path
            )
            vectors = loader.return_vectormeta_format()
            await assert_cancelled(request)

            # Remove the temporal chunks
            try:
                subprocess.run(['rm', '-r', tmp_path], check=True)
            except:
                pass
            await assert_cancelled(request)
            return vectors

        elif ext in ('.csv', '.xlsx'):
            loader = TabularLoader(file_path, ext)
            vectors = loader.return_vectormeta_format()
            await assert_cancelled(request)
            return vectors

        elif ext == '.hwp':
            documents: list[Document] = self.load_documents(file_path, **kwargs)
            await assert_cancelled(request)
            chunks: list[Document] = self.split_documents(documents, **kwargs)
            await assert_cancelled(request)
            vectors: list[dict] = self.compose_vectors(file_path, chunks, **kwargs)
            return vectors

        elif ext in ('.hwpx'):
            return await self.hwpx_processor(request, file_path, **kwargs)

        else:
            documents: list[Document] = self.load_documents(file_path, **kwargs)
            await assert_cancelled(request)

            chunks: list[Document] = self.split_documents(documents, **kwargs)
            await assert_cancelled(request)

            vectors: list[dict] = self.compose_vectors(file_path, chunks, **kwargs)
            return vectors
```

**ì„¤ëª…:**

* `ext = os.path.splitext(file_path)[-1].lower()`: íŒŒì¼ ê²½ë¡œì—ì„œ í™•ì¥ìë¥¼ ì¶”ì¶œí•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤. í™•ì¥ìì— ë”°ë¼ì„œ ì í•©í•œ ì²˜ë¦¬ ë¡œì§ì„ ì„ íƒí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
* `if ext in ('.wav', '.mp3', '.m4a'):`: ì˜¤ë””ì˜¤ íŒŒì¼ì¸ ê²½ìš°, ì˜¤ë””ì˜¤ ì „ìš© ì²˜ë¦¬ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  * `tmp_path = "./tmp_audios_{}".format(os.path.basename(file_path).split('.')[0])`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë””ë ‰í† ë¦¬ëŠ” ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì €ì¥í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
  * `if not os.path.exists(tmp_path): os.makedirs(tmp_path)`: ì„ì‹œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
  * `loader = AudioLoader(...)`: `AudioLoader` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ `req_url`ê³¼ `req_data`ëŠ” ìŒì„± ì¸ì‹ ëª¨ë¸ì— ëŒ€í•œ ìš”ì²­ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
  * `vectors = loader.return_vectormeta_format()`: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° ë©”íƒ€ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.
  * `await assert_cancelled(request)`: ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
  * `subprocess.run(['rm', '-r', tmp_path], check=True)`: ì„ì‹œ ì˜¤ë””ì˜¤ ì²­í¬ ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•˜ì—¬ ì •ë¦¬í•©ë‹ˆë‹¤.
  * `return vectors`: ìƒì„±ëœ ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* `elif ext in ('.csv', '.xlsx'):`: CSV ë˜ëŠ” XLSX íŒŒì¼ì¸ ê²½ìš°, í‘œ í˜•ì‹ ì „ìš© ì²˜ë¦¬ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  * `loader = TabularLoader(file_path, ext)`: `TabularLoader` ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ í‘œ í˜•ì‹ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
  * `vectors = loader.return_vectormeta_format()`: í‘œ í˜•ì‹ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° ë©”íƒ€ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜ëœ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.
  * `await assert_cancelled(request)`: ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
  * `return vectors`: ìƒì„±ëœ ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* `elif ext == '.hwp': ...`: HWP íŒŒì¼ì¸ ê²½ìš°, HWP ì „ìš© ì²˜ë¦¬ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  * `documents: list[Document] = self.load_documents(file_path, **kwargs)`: HWP íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  * `await assert_cancelled(request)`: ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
  * `chunks: list[Document] = self.split_documents(documents, **kwargs)`: ë¬¸ì„œ ê°ì²´ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
  * `await assert_cancelled(request)`: ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
  * `vectors: list[dict] = self.compose_vectors(file_path, chunks, **kwargs)`: ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  * `return vectors`: ìƒì„±ëœ ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* `elif ext == '.hwpx': ...`: HWPX íŒŒì¼ì¸ ê²½ìš°, HWPX ì „ìš© ì²˜ë¦¬ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  * `return await self.hwpx_processor(request, file_path, **kwargs)`: `HwpxProcessor` ì¸ìŠ¤í„´ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ HWPX íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
* `else: ...`: ê·¸ ì™¸ì˜ íŒŒì¼ í˜•ì‹ì¸ ê²½ìš°, ì¼ë°˜ì ì¸ ë¬¸ì„œ ì²˜ë¦¬ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  * `documents: list[Document] = self.load_documents(file_path, **kwargs)`: ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  * `await assert_cancelled(request)`: ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
  * `chunks: list[Document] = self.split_documents(documents, **kwargs)`: ë¬¸ì„œ ê°ì²´ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
  * `await assert_cancelled(request)`: ìš”ì²­ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì·¨ì†Œëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
  * `vectors: list[dict] = self.compose_vectors(file_path, chunks, **kwargs)`: ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  * `return vectors`: ìƒì„±ëœ ë²¡í„° ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

**ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸:**

* `**kwargs` í™œìš©: `__call__` ë©”ì„œë“œì— ì „ë‹¬ë˜ëŠ” `**kwargs`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ `load_documents`, `split_documents`, `compose_vectors`ë¡œ ì „íŒŒë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¬¸ì„œ ì²˜ë¦¬ ì „ ê³¼ì •ì— ê±¸ì³ ë™ì ì¸ ì„¤ì •ì„ ì£¼ì…í•˜ëŠ” í†µë¡œë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, API ìš”ì²­ìœ¼ë¡œë¶€í„° íŠ¹ì • íŒŒë¼ë¯¸í„°ë¥¼ ë°›ì•„ `kwargs`ë¡œ ì „ë‹¬í•˜ê³ , ì´ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ `PdfPipelineOptions`ì˜ ì¼ë¶€ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ `compose_vectors`ì—ì„œ íŠ¹ì • ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€/ì œì™¸í•˜ëŠ” ë“±ì˜ ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### âœ¨ ì‚¬ìš©ì ì •ì˜ í¬ì¸íŠ¸

* **ë©”íƒ€ í•„ë“œ í™•ì¥**: `GenOSVectorMeta`ì™€ `Builder` í´ë˜ìŠ¤ì— í•„ë“œ ì¶”ê°€
* **í˜ì´ì§€ ì²˜ë¦¬ ë¡œì§ ìˆ˜ì •**: `set_page_info` íŒŒë¼ë¯¸í„° ì¡°ì •
* **ì²­í¬ ë¶„í•  ì»¤ìŠ¤í„°ë§ˆì´ì§•**:  `HybridChunker` íŒŒë¼ë¯¸í„° ê¸°ì¤€ê°’ ìˆ˜ì •
* **ë¬¸ìì—´ ë³€í™˜**: `NaN`, `None` ë“± ê°’ì€ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬

### âœ… ìœ ì§€ë³´ìˆ˜ íŒ

* Pydantic `extra='allow'` ì„¤ì •ìœ¼ë¡œ í•„ë“œ ë³€ê²½ì´ ìœ ì—°í•˜ê²Œ í—ˆìš©ë¨
* Builder íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ í•„ë“œ ì„¤ì • ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê³  ìœ ì§€ë³´ìˆ˜ë¥¼ ë‹¨ìˆœí™”

***

ì´ì™€ ê°™ì´ ì½”ë“œ ì¡°ê°ê³¼ í•¨ê»˜ ì„¤ëª…ì„ ë³´ë©´ì„œ `DocumentProcessor`ì™€ `GenOSVectorMetaBuilder`ì˜ ì‘ë™ ë°©ì‹ê³¼ ì‚¬ìš©ì ì •ì˜ ì§€ì ì„ íŒŒì•…í•˜ì‹œë©´, ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜ì •í•˜ê³  í™•ì¥í•˜ì‹¤ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
